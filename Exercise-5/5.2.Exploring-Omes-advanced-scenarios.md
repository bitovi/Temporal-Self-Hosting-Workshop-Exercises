# 5.2 Exploring Omes advanced scenarios
We'll now create a worker and scenario separately which will allow us to run load simulations that require horizontal worker fleet scaling to solve.


Scale your services:
```bash
kubectl scale deployment/my-temporal-frontend --replicas=1;
kubectl scale deployment/my-temporal-history --replicas=3;
kubectl scale deployment/my-temporal-matching --replicas=1;
kubectl scale deployment/my-temporal-worker --replicas=1;
```


## state_transitions_steady
First create a worker:
```bash
go run ./cmd run-worker \
    --scenario state_transitions_steady \
    --language go \
    --run-id state-transitions-test-run \
    &> ../logs/state_transitions_steady_worker.log
```


Next begin the simulation:
```bash
go run ./cmd run-scenario \
    --scenario state_transitions_steady \
    --duration 1m \
    --option state-transitions-per-second=30 \
    --run-id state-transitions-test-run \
    &> ../logs/state_transitions_steady_scenario.log
```


This is a start, but let's actually push the Temporal service!

```bash
go run ./cmd run-scenario \
    --scenario state_transitions_steady \
    --duration 3m \
    --option state-transitions-per-second=300 \
    --run-id state-transitions-test-run \
    &> ../logs/state_transitions_steady_scenario.log
```


Create 2 more workers and rerun the test

### Discussion
Two very important insights can be found in the Grafana dashboard. Set the time scale to the last 15 minutes to capture both of our tests. 


1. What happened to the schedule to start latency and what does this tell us?
    - Hint: Consider how the backlog was handled
2. What happened to our state transitions



Stop the two workers and run the following to delete the workflow event history

```bash
temporal operator namespace delete default --yes;
temporal operator namespace create default;
```

## throughput_stress

Scale your services:
```bash
kubectl scale deployment/my-temporal-frontend --replicas=1;
kubectl scale deployment/my-temporal-history --replicas=3;
kubectl scale deployment/my-temporal-matching --replicas=1;
kubectl scale deployment/my-temporal-worker --replicas=1;
```


Create a worker fleet of 3

```bash
go run ./cmd run-worker \
    --scenario throughput_stress \
    --language go \
    --run-id throughput-stress-test-run \
    &> ../logs/throughput_stress_worker.log
```

Create the search attribute that the test will use

```bash
temporal operator search-attribute create --name ThroughputStressScenarioId --type Keyword
```

```bash
go run ./cmd run-scenario \
    --scenario throughput_stress \
    --option sleep-activity-json=@../sleep.json \
    --run-id throughput-stress-test-run \
    --duration 2m \
    &> ../logs/throughput_stress_scenario.log
```


### Discussion
1. In the Temporal UI add the state transitions column
    - Look at the number of workflows that we ran, then look at the number of state transitions that we ran
        - Why is workflows started and completed **not** a good metric to base performance on based on what you see?
2. What does the continue-as-new state mean?
    - Why would we use this state?
