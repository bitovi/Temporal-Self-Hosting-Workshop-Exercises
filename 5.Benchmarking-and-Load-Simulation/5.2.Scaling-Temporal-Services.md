# Scaling Temporal Services
In the last section we spent time getting more familiar with metric data visualization and leanered about what consistency means in our current system. It's now time to make changes to our system capacity and understand how to interpret how the server responds.


## Exercise 1 - Add load to the system
In this exercise we want to really push the amount of load the test is generating and see how our Temporal service responds. We'll explore what we can determine with our dashboards and UI and finally we'll manually stop any remaining workflows that became stuck during our test.

You have one question to answer:
1. What are the indicators that something is wrong and our Temporal Service isn't keeping up?

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```


### Answers
1. In the Temporal UI the "Running" workflows are stacking up faster than the "completed"
2. In the Temporal Server Metrics dashboard, within the **Workflow tasks** and **Activities** panels, up until this point the number of completed tasks has always been at least double the started value, this means that on we complete two activities for every 1 created. With this test however, the started amount has begun to exceed the completed values, resulting in backlogging of requests. In a live environment this **could** be dealt with as the worker given enough time will execute and resolve the backlog of items once requests decrease, however this is still highlighting what high worker stress would look like.


It's likely that you now have stuck workflows since the worker is stopped once the test finishes. Scroll to the bottom of this page for information on how to [terminate stuck workflows](#stuck-workflows). Before doing so make a note of how many workflows had to be terminated.


## Exercise 2 - Increase the replica count of all primary services
Let's now increase the amount of replicas we have for our Frontend, History and Matching service. Doing so will result in a shard rebalance that we will want to allow to normalize before proceeding. 

Perform the following command:
```bash
kubectl scale deployment my-temporal-history --replicas=2;
kubectl scale deployment my-temporal-matching --replicas=2;
```

Within the **Temporal Server Metrics** dashboard wait until the **Shard Rebalancing** and **Shard Distribution** panels normalize, it should take 2 minutes. Afterward, let's redo that test and see if anything changes.

Let's test whether increasing the replicas of our crucial services results in better performance.

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```

**Answer:** Yes and no:
_Yes:_ Looking at most of the operation metrics, we see a more gradual rise then fall instead of the jerky peak, hold, dip pattern before. This indicates that workflows/activities are being worked on in parallel resulting in smoother overall operations.
_no:_ We still see the build up of a backlog, this is because of our local machine acting as a worker is simply not able to execute on the workflows fast enough to finish the test.

What we can infer from this is that horizontal scaling does solve some problems, such as Temporal through-put, but if the number of workers isn't scaling to meet demand than it doesn't matter how much we scale Temporal up, the worker layer becomes the bottleneck.


## Stuck workflows
Because our local machine is being treated as a worker and there is a time limited duration for the test, it is possible that the test concludes before the workflows can all finish. In this case we can terminate those tasks by running the following:

```bash
temporal workflow terminate --query "WorkflowType=\"kitchenSink\""
```

Let's stop and consider what this means

## Exercise conclusion

In this exercise, you explored the practical impact of scaling core Temporal services and adjusting cluster architecture on the system’s capacity and resilience. By testing your cluster under heightened loads and modifying resource allocation, you:

- Identified Bottlenecks Under Heavy Load: By dramatically increasing the workflow generation rate, you witnessed firsthand what system strain looks like, such as running workflows outpacing completions and a clear backlog building in the Temporal UI, indicating when the worker infrastructure cannot keep up.
- Used Metrics to Spot Resource Constraints: You analyzed Grafana dashboards to pinpoint where the system was lagging, observing key changes in workflow/activities task ratios, shard distribution, and latency. This process highlighted the need to correlate dashboard signals with “real” operational slowdowns.
- Scaled Out Core Services: By increasing the replica counts of Frontend, History, and Matching services, you observed both the benefits and limits of horizontal scaling. Work was distributed more evenly across pods, but new bottlenecks emerged, such as shard-level throttling or worker limitations, illustrating that not all performance issues can be solved by simply adding replicas.
- Understood the Role of Shards and Capped Scalability: You discussed the impact of shard count and server architecture, learning why simply increasing shards doesn’t always provide linear improvements and how improper sizing can impose self-inflicted constraints.
- Worked Through Hung Workflows and Remediation: You experienced the downstream effects of heavy load and incomplete test runs—such as workflows stuck in progress—and practiced remediating these situations by manually terminating workflows, emphasizing the importance of dedicated benchmarking resources.
- Prepared for System-Level Risks: You also handled system-level risks like potential out-of-memory scenarios, reinforcing the value of cluster monitoring and recovery strategies as part of operational excellence.

Key Takeaways:

1. Scaling Temporal requires more than just adding compute; understanding the interplay between workflow throughput, worker capacity, shard distribution, and live system metrics is essential for effective architectural decisions.
1. Observability enables not just detection of slowdowns or errors, but a root-cause mindset: Knowing why the backlog forms or where capacity is being exhausted is the difference between short-term fixes and lasting improvements.
1. Realistic load and scaling experiments are essential for validating “real world” readiness—highlighting that resolving bottlenecks demands holistic awareness of both application and infrastructure layers.

Armed with this hands-on scaling experience, you now possess deeper operational insight into how Temporal’s architecture responds to high-stress conditions, and how to interpret the metrics and behaviors that surface under load.


