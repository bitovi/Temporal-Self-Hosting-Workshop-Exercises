# 5.3: Benchmarking
Benchmarking is important to Temporal because it allows the team to systematically measure and evaluate the performance, scalability, and reliability of the Temporal platform. By running benchmarks, you can identify bottlenecks, optimize core components, and ensure the system meets performance expectations for real-world workloads.


In this exercise we are going to work with Temporal's benchmarking tool as it's easy to setup and get things running.

From the project root run the following within a terminal window/tab:
```bash
kubectl apply -f ./benchmark-deployment.yaml
```
This will create two deployments: One that will connect to the Temporal service and act as a worker and the other which will create workflow requests that the first worker will act on.

Let's go back into the Grafana dashboard, on the left panel select **Explore** and enter the following:

```
sum(rate(state_transition_count_count{namespace="default"}[2m]))
```

It may be tempting to only look at workflow success vs workflow failure but this isn't always the case. Temporal is durable and execution often times have retry logic, as a result success vs failure doesn't always tell the whole story of how the Temporal service is performing. This is going to measure the **number of state transitions** which represents Temporal writing to its persistence backend, which is a reasonable proxy of how much work Temporal itself is doing. This is a handy metric to leave up and follow as we continue through this section.

## Exercise 1 - Increase the number of workflow requests

Run the following command:

```bash
kubectl scale deployment benchmark-soak-test --replicas=2;
```

Take some time to find the answers to the following:
- What indicators are there that the increased number of requests aren't being executed on by enough worker compute?
- What layer of Temporal would need to be increased to fix this issue?

### Answers
- What indicators are there that the increased numbed of requests aren't being executed on by enough worker compute?
    1. In **Service Client - All > Errors by Operation** we saw that matching has a spike in errors, specifically it wasn't able to find enough worker to meet demand
    2. In the UI we say a consistent stack of +10n workflows, the worker was only able to get complete about 10/second but ~20/second were being requested




## Exercise conclusion

To ensure our cluster stability let's now reduce the system load to ensure our cluster doesn't become unstable. 

```bash

kubectl scale deployment benchmark-soak-test --replicas=0;
kubectl scale deployment benchmark-workers --replicas=0;
```