# 4.3: Simulating failures and viewing retry and alerting behavior

In our last exercise we took time to read through and understand the first responder dashboard: **Temporal Server Metrics**. This should always be the first place we look when we want to begin diagnosing or investigating our Temporal service health and performance.

Up until now the system _should_ be running at peak performance, at least within reason of our local machine, now we are going to force change and see what the results are.


## Exercise setup
In your Grafana UI, on the top right click the down pointing arrow beside the refresh button, in the dropdown menu select **auto**. To the left of that ensure the timescale selected is **Last 5 minutes**.


## Exercise 1 - Create an alert
Let's begin by creating  Prometheus alert that will sound off when our system performance becomes degragaded. Prometheus alerts aren't a fully-fledged notfication solution but it does provide a warning for operators when something in their self-hosted Temporal service isn't right.


1. In the left hand panel open the **Alerting > Alert Rules**
2. On the top right click the **New Alert Rule** button
3. Enter the alert rule name:  **Service Availability below 75%**
4. In the define query and alert condition
    - Select **code** and paste the following: `100 - (sum(rate(service_errors[2m]) OR on() vector(0)) / sum(rate(service_requests[2m])) * 100)`
    - Click **Options** and change Time Range to `now-1m to now`, Max data points to `43200` and interval to `1s`
5. Within the expressions tab
     - Within the **B Reduce** panel, remove the **B Reduce** panel by clicking the garbage can on the right
     - Within the **C Threshold** panel, update the input to A, update the **IS ABOVE** evaluation to **IS BELOW** and set the value to 75
6. Within the **Set evaluation behavior** tab
    - Click `+ New folder` and give it the name `Service_Availability`
    - Click `+ New evaluation group` and give it the name `Service_Availability_Group`, Set the Evaluation interval to 10s
7. Within the **Configure label and notifications** tab
    - Set the **Contact point** to `grafana-default-email`
8. On the top right click `Save rule and exit`


This rule will watch the same value as the **Temporal Server Metrics dashboard > Service Availability** panel, if the value drops below 75% it will trigger this alert. Within the **Alerting > Alert Rules** open up the `Service_Availability > Service_Availability_Group`, you can see our newly created rule and assuming your Temporal service is stable and running, we will see the tags `1 normal | 10s`.

Let's now break some things and see how our dashboards respond!


## Exercise 2 - Simulate Pod Disruption
Let's begin by simulating failure within our frontend service, this will trigger our alert rule and we will see the issue witihin our **Temporal Server Metrics dashboard > Service Availability** panel.


Begin by opening the **Temporal Server Metrics dashboard > Service Availability** panel, we want to see the failure as it happens

Let's begin by scaling our frontend pod down to 0 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=0
```

Keep an eye on your Grafana **Temporal Server Metrics** dashboard, we can see that the panels have already begun to change.

1. **Temporal > Service Availability** panel: We can see that availability has dropped from 100 to NaN
2. **Overview - All > Service Requests** panels: We can see that the number of requests has dropped from a consistent ~1.5 calls to 0. Notice that the error rate hasn't increased, this is because the simulation isn't causing failure but instead pod disruption which has stopped the requests from happening all together. As an operator if we were to see an issue like this during normal operations, where requests have stopped but errors aren't coming up, we'd know that pod stability is to blame.

> [!TIP]
> Temporal Server Metrics is a starting point, in a real-life event we would want to open the Frontend dashboard and dig in to the issue deeper.

Before moving on, let's go back to our rule and see if that too has changed.. and it has! It has changed from `1 normal | 10s` to `1 firing | 10s`, so our alerting rule is working as expected and had we setup a true target, for example Slack, we would be receiving notifications about our Frontend service requests meeting our alert threshold.

### Exercise clean-up

Let's now scale our frontend pod down back to 1 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=1
```

Back in our dashboards (wait ~2 minutes) we can see that our Service Availability has returned to the expected 100%


## Exercise 3 - Simulate failure
In our next exercise we will simulate failure by scaling the persistence database down to 0. In the real world there are many more potential failures could happen such as OOM / compute exhaustion, etc.


Begin by opening the **Temporal Server Metrics dashboard > Persistence - All** panels, we want to see the failure as it happens

```bash
kubectl scale statefulset my-temporal-cassandra --replicas=0
```

Unlike before, a lot is about to happen so let's look what is going on in detail.

### Error spikes 
The error spikes are growing and fast, this is caused because the Temporal services are no longer able to reach the persistence database to update their status. Before when we brought the frontend pod down, it didn't cause error because the Temporal services, such as frontend, history, etc, are considered loosely coupled, therefore the frontend not being available is a problem, but is not a "show stopper". The services being unable to update their status results in this level of error because state is the primary means of cordination between the services. This also showcases just how important the persistence layer is to the Temporal server, the way the Temporal service reacted to this simulation should highlight the importance of an optimized persistence layer being available to the Temporal services.

### Requests spikes
When things are going good we see consistent event requests, when things go wrong we see spikes in events. This is because the Temporal services are attempting to initiate an event, such as a event state update, failing and retrying. The failures will eventaully normalize, so always ensure you consider both event requests and errors to understand the health of a particular service. 

Take 5 minutes to explore the other panels and dashboards. Notice how many things are going on from this simple failure.

### Exercise clean-up
When you're ready scale the persistence database back to 1. Open your terminal window / tab and enter the following:

```bash
kubectl scale statefulset my-temporal-cassandra --replicas=1
```

Look back to your **Temporal Server Metrics dashboard > Persistence availability panel**, even though we have given this time we still aren't seeing the graph move, so what's going on? In this case it is because the persistence database that we scale back up doesn't have persistent state, so our data was deleted when we scaled to 0. If we look at our **Temporal Server Metrics dashboard > Persistence - All** panels we also notice that the number of requests and errors have come down but aren't 0.. this is because the database is being connected to but failures are still occuring during certain (write) events, so it **looks** as though things are fixed, but that is a red herring. Something is still wrong..


Within we can see that the history pod is complaining about not being able to find the keyspace "Temporal". Again this issue is a direct result of the database not having persistence, we can fix this by running the following:

```bash
kubectl apply -f ./4.Observability-and-Monitoring/cassandra-schema-job.yaml
```

With that our persistence database should now be available once again.. but something still seems wrong.. looking at our pods, we can see instability in our history pod.. what's going on now?!


## Exercise 4 - Fixing the History service
From Grafana we can see there is an issue, look at the **Persistence - All** panels, even though the persistence datastore is available and has returned to 100% availablity, we're still seeing errors, just not what the issue is specifically. In this case we would need to dig into pod logs to understand what the issue is.

To diagnose this further we would need to review our history pods logs. Open your terminal window / tab and enter the following:
```bash
kubectl logs deployment/my-temporal-history --follow 
> "level":"fatal","ts":"2025-07-07T17:50:16.016Z","msg":"failed to start ringpop","error":"exhausted all retries: join duration of 39.294710601s exceeded max 30s"
```

From our failure simulation we've accidentally lost the ringpop address for history. This is the way that the inner Temporal services communicate, without it our history pod will be unstable. Let's first verify that this is an issue. 


Open your terminal window / tab and enter the following:
```bash
kubectl port-forward deployment my-temporal-frontend 7233 7233
```

Open a separate terminal window / tab and enter the following:
```bash
tctl admin cluster describe
```

Scroll through the output for `"role": "history"`, notice that unlike the other services our history service doesn't have an identity. 

To fix this we need to cycle the history pod. 

```
kubectl scale deployment my-temporal-history --replicas=0
# Wait until the history pod is termianted
kubectl scale deployment my-temporal-history --replicas=1
```

Give the deployment a moment to come back up and stablize, then rerun:
```bash
tctl admin cluster describe
```


The history pod should now be back in the ringpop addresses and all services will now be stable.

> [!TIP]
> If other services lack an identity simply cycle them to have the services' ringpop address added back to the persistence database and restore functionality.


## Exercise conclusion

In this exercise, you practiced real-world operations to stress-test your Temporal deployment, simulate service failures, and validate your observability setup. By intentionally disrupting core services and observing system reactions, you gained first-hand experience with:

1. Setting up effective alerting: You created a Prometheus-based alert to proactively notify operators of declining service availability, ensuring you are informed as soon as service health degrades.
1. Diagnosing service interruptions: By scaling down the frontend pod, you saw how the Temporal dashboards reflect pod disruptions and how alerts can be triggered by transient outages even before errors accumulate.
1. Recognizing and interpreting error spikes: You simulated persistence database failure and observed how instantly this propagates throughout Temporal, reflected in error rates, request spikes, and dashboard anomalies.
1. Identifying misdirection and remediation: You saw that dashboards may sometimes signal apparent recovery while underlying issues remain, such as persistent write errors or missing database schema, and learned to use logs and operational tools (tctl, pod logs) for deeper investigation.
1. Service recovery procedures: You practiced restoring pods and reapplying schema to recover from outages, and learned how to restore lost service identity in the Temporal cluster.

Key Takeaways:

1. Dashboards Offer Clues, Not Complete Answers: The Temporal Server Metrics dashboards are crucial for surface-level detection and correlation, but true root cause analysis may require deeper dives into logs and service internals.
1. Alerting Completes the Monitoring Loop: Well-configured alerts help bridge the gap between passive observability and active operations, enabling rapid response to emergent issues.
1. Operational Readiness is Proactive: Practicing simulations and familiarizing yourself with both expected and unexpected Temporal failure modes is key to quickly restoring service—and confidence—when an incident happens “for real.”

By experimenting with pod disruptions, persistence outages, and service fixes in a controlled environment, you’ve built confidence in both your technical tooling and your process for incident response. You’re now better equipped to diagnose, respond to, and recover from a range of Temporal platform incidents.

This concludes the out-of-the-box metrics Temporal recommendations, next let's look into what [additional custom metrics](./4.4.Additional-Metrics.md) you should have based on Bitovi's hands-on work with Temporal.

