# 4.3: Simulating failures and viewing retry and alerting behavior

In our last exercise we took time to read through the first responder dashboard: **Temporal Server Metrics**. This should always be the first place we look when we want to begin diagnosing or investigating our Temporal service health and performance.


## Exercise setup
In your Grafana UI, on the top right click the down pointing arrow beside the refresh button, in the dropdown menu select **auto**. To the left of that ensure the timescale selected is **Last 5 minutes**.


## Exercise 1 - Create an alert
Let's begin by creating  Prometheus alert that will sound off when our system performance becomes degragaded. Prometheus alerts aren't a fully-fledged notfication solution but it does provide a warning for operators when something in their self-hosted Temporal service isn't right.


1. In the left hand panel open the **Alerting > Alert Rules**
2. On the top right click the **New Alert Rule** button
3. Enter the alert rule name:  **Service Availability below 75%**
4. In the define query and alert condition
    - Select **code** and paste the following: `100 - (sum(rate(service_errors[2m]) OR on() vector(0)) / sum(rate(service_requests[2m])) * 100)`
    - Click **Options** and change Time Range to `now-1m to now`, Max data points to `43200` and interval to `1s`
5. Within the expressions tab
     - Within the **B Reduce** panel, remove the **B Reduce** panel by clicking the garbage can on the right
     - Within the **C Threshold** panel, update the input to A, update the **IS ABOVE** evaluation to **IS BELOW** and set the value to 75
6. Within the **Set evaluation behavior** tab
    - Click `+ New folder` and give it the name `Service_Availability`
    - Click `+ New evaluation group` and give it the name `Service_Availability_Group`, Set the Evaluation interval to 10s
7. Within the **Configure label and notifications** tab
    - Set the **Contact point** to `grafana-default-email`
8. On the top right click `Save rule and exit`


This rule will watch the same value as the **Temporal Server Metrics dashboard > Service Availability** panel, if the value drops below 75% it will trigger this alert. Within the **Alerting > Alert Rules** open up the `Service_Availability > Service_Availability_Group`, you can see our newly created rule and assuming your Temporal service is stable and running, we will see the tags `1 normal | 10s`.

Let's now break some things and see how our dashboards respond!


## Exercise 2 - Simulate Frontend Pod Disruption
Let's begin by simulating failure within our frontend service, this will trigger our alert rule and we will see the issue witihin our **Temporal Server Metrics dashboard > Service Availability** panel.


Begin by opening the **Temporal Server Metrics dashboard > Service Availability** panel, we want to see the failure as it happens

Let's begin by scaling our frontend pod down to 0 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=0
```

Keep an eye on your Grafana **Temporal Server Metrics** dashboard, we can see that the panels have already begun to change.

1. **Temporal > Service Availability** panel: We can see that availability has dropped from 100 to NaN
2. **Overview - All > Service Requests** panels: We can see that the number of requests has dropped from a consistent ~1.5 calls to 0. 


> [!TIP]
> Temporal Server Metrics is a starting point, in a real-life event we would want to open the Frontend dashboard and dig in to the issue deeper.

Let's go back to our rule and see if that too has changed.. and it has! It has changed from `1 normal | 10s` to `1 firing | 10s`, so our alerting rule is working as expected and had we setup a true target, for example Slack, we would be receiving notifications about our Frontend service requests meeting our alert threshold.

**Question:** The frontend pod is how external services, such as workers, contact Temporal. If the frontend service was to be disrupted, what could we expect to happen?

### Exercise clean-up

Let's now scale our frontend pod down back to 1 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=1
```

## Topic: Datastore Failure

Unlike failures in Temporal’s individual services, which are generally loosely coupled and resilient, a failure of the persistence datastore is a far more disruptive scenario. The persistence datastore is responsible for storing the state of workflows, activities, tasks, and services. If this datastore (for example, Cassandra) becomes unavailable, it can create widespread issues across the entire Temporal platform.

This type of incident would quickly become visible in our Grafana dashboards through two primary indicators:

### Error Spikes

When the persistence datastore is unavailable, Temporal services cannot update or retrieve the state they require. This results in a sharp increase in errors across all services.

Previously, when we brought down a frontend pod, the system experienced a loss of frontend functionality but continued operating otherwise because Temporal services (such as frontend and history) are loosely coupled. In contrast, losing the persistence datastore affects all services simultaneously—since they rely on this shared layer for coordination. This scenario produces widespread errors and underscores the critical role of a reliable, high-performance persistence layer for Temporal’s overall stability.

### 2. Request Spikes

During normal operations, request rates are steady and predictable. However, when the persistence layer fails, services repeatedly attempt state updates and retries—leading to noticeable spikes in event requests. As failures persist and services continue to retry, these request rates rise sharply. Eventually, the volume of failures may stabilize, but during the incident, both error counts and request spikes must be monitored to get an accurate picture of service health.


## Exercise conclusion
In this exercise, you practiced real-world operations to stress-test your Temporal deployment, simulate service failures, and validate your observability setup. By intentionally disrupting core services and observing system reactions, you gained first-hand experience with:

1. Setting up effective alerting: You created a Prometheus-based alert to proactively notify operators of declining service availability, ensuring you are informed as soon as service health degrades.
1. Diagnosing service interruptions: By scaling down the frontend pod, you saw how the Temporal dashboards reflect pod disruptions and how alerts can be triggered by transient outages even before errors accumulate.
1. Recognizing and interpreting error spikes: We spoke about persistence database failure andhow instantly errors would propagates throughout Temporal, reflected in error rates, request spikes, and dashboard anomalies.

Key Takeaways:

1. Dashboards Offer Clues, Not Complete Answers: The Temporal Server Metrics dashboards are crucial for surface-level detection and correlation, but true root cause analysis may require deeper dives into logs and service internals.
1. Alerting Completes the Monitoring Loop: Well-configured alerts help bridge the gap between passive observability and active operations, enabling rapid response to emergent issues.
1. Operational Readiness is Proactive: Practicing simulations and familiarizing yourself with both expected and unexpected Temporal failure modes is key to quickly restoring service—and confidence—when an incident happens “for real.”

By experimenting with pod disruptions, persistence outages, and service fixes in a controlled environment, you’ve built confidence in both your technical tooling and your process for incident response. You’re now better equipped to diagnose, respond to, and recover from a range of Temporal platform incidents.


This concludes the out-of-the-box metrics Temporal recommendations, next let's look into what [additional custom metrics](./4.4.Additional-Metrics.md) you should have based on Bitovi's hands-on work with Temporal.

