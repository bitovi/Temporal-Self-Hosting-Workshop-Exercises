# 4.2.2: Additional Dashboards

In this section we will go through some of the additional dashboards operators have available to them out-of-the-box, what the importance is and how we would use them in a real-world deployment. 

## Cluster Monitoring for Kubernetes Dashboard

This dashboard gives operators a comprehensive, real-time overview of the resource consumption and health of the entire Kubernetes cluster running Temporal. Its purpose is to ensure adequate capacity, spot bottlenecks, and correlate cluster-level constraints with Temporal service behavior.

- Cluster memory usage
  - Displays the overall percentage and absolute amount of memory used versus available across all nodes, alerting operators to memory saturation risks that could impact pod scheduling and service stability.
- Cluster CPU usage (2m avg)
  - Tracks cluster-wide CPU utilization averaged over 2 minutes, helping catch overloads, detect runaway processes, and inform scaling or resource allocation decisions proactively.
- Cluster filesystem usage
  - Shows how much disk space is consumed on cluster storage, helping spot trends towards full filesystems that can cause pod eviction or database issues.
- Container Memory Usage
  - Breaks down memory consumption per container, enabling pinpointing of memory-heavy pods and identifying memory leaks or abnormal allocation patterns per Temporal component.
- Container CPU Usage
  - Visualizes CPU usage per container, making it easy to find unexpected hot spots or workload imbalances tied to specific services.
- Network I/O Pressure / Container Network I/O
  - Tracks overall and container-level network throughput and congestion, essential for diagnosing situations where network saturation could contribute to high latency, failed client calls, or workflow delays.

This dashboard is critical for SREs and operators to correlate pod-level or node-level issues (such as OOM kills, CPU throttling, or disk exhaustion) with higher-level Temporal service performance, root causing environment-based incidents, and capacity planning for future cluster expansion.



## Frontend Service Dashboard

This dashboard provides deep visibility into the Temporal Frontend service and its most critical dependencies, enabling operators to monitor request rates, error patterns, and latency across API operations and key backend service clients. It is essential for understanding the health, performance, and reliability of Temporal’s main request handling layer.

- Frontend Requests Per Operation (requests, errors)
  - Tracks the volume and error count of API calls grouped by operation, allowing rapid identification of failing or degraded UI/API endpoints.
- Frontend Requests: PollActivityTaskQueue & PollWorkflowTaskQueue
  - Focuses specifically on polling rates for activity and workflow task queues—the backbone of worker connectivity and workflow progress—helping diagnose stuck or overloaded queues.
- Frontend Errors Break Down
  - Visualizes errors by operation or error type, which helps prioritize incident response efforts on the most disrupted workflow paths.
- Frontend Latency (including PollActivityTaskQueue & PollWorkflowTaskQueue)
  - Shows end-to-end response times for API calls, with breakdowns for high-impact ops. Rising latencies can point to real-time performance regressions or backend slowdowns.
- Persistence Request vs Errors / Requests Per Operation
  - Monitors the volume and errors of database-layer operations (like listing namespaces, saving cluster membership, or initializing system namespaces), surfacing underlying storage or persistence failures that would impact API stability.
- Persistence Errors Break Down & Latency
  - Provides detailed error and latency metrics for backend operations, invaluable for tracing root causes of platform-wide outages or performance dips to the persistence layer.
- History Client Requests vs Errors and Per Operation
  - Tracks request and error rates for all interactions with the History service, which manages workflow state. Patterns here highlight emerging issues in workflow execution pipelines.
- History Client Errors By Operation & Latency
  - Exposes operation-specific bottlenecks and performance issues in workflow history management.
- Matching Client (Requests/Errors/Latency by Operation)
  - Monitors communications with the Matching service, which is critical for task queue throughput. Per-operation details (e.g., productive and failed polls, endpoint listing) provide actionable insight for debugging worker registration and queue delays.

This dashboard is vital for SREs and platform engineers to pinpoint where breakdowns are happening in the Temporal request flow—from API ingress through service internal calls and persistence. It streamlines the troubleshooting process by highlighting exactly which operations or services are underperforming or failing, enabling fast root cause identification and targeted remediation. It's also helpful for proactively managing scaling, as request and latency charts reveal when workload exceeds current capacity or backend responsiveness.

## History Service Dashboard

This dashboard provides detailed observability into the core Temporal History service, which manages workflow state transitions, task scheduling, event history, and temporal sharding. It helps SREs and operators monitor correctness, efficiency, and bottlenecks in workflow execution, transfer and timer task processing, cache health, and persistence interactions.

- History Service Requests, Errors, and Latency (including per-operation and for StartWorkflowExecution)
  - Tracks API calls received by the History service. Per-operation breakdowns (especially for starting workflows) help you pinpoint failing or slow user-facing actions and measure workflow creation health/performance.
- Transfer Tasks Processing (requests/errors, latency, by operation)
  - Surfaces the status and latency of transfer task processing, which is fundamental to delivering signals, activity scheduling, workflow starts/closures, and task queue interactions. Spikes or delays here reveal issues in transferring work across services or to worker clients.
- Timer Tasks Processing (requests/errors, latency, by operation)
  - Focuses on time-driven (delayed) workflow actions like retries, scheduled events, and timers. High error rates, queueing, or latency can indicate broad disruptions in “time-based” workflow behaviors—key for detection of missed deadlines or stuck execution.
- Mutable State Cache (requests, cache misses, latency, per operation)
  - Monitors in-memory caching of active workflow state and the rate of cache misses. Frequent misses or high load/latency may point to state access inefficiencies, driving unnecessary backend reads and degrading workflow performance.
- Events Cache (overview, put/get, latency)
  - Reports on Temporal’s event history caching layer. Problems here may impact workflow replay performance or recovery after failures, showing up as latency spikes or increased backend load.
- Workflow Task Insights (breakdown, sticky vs non-sticky, failures, empty tasks)
  - Analyzes assignment and success rates of workflow tasks. High failure or empty counts help detect workflow logic bugs, worker issues, or misconfigurations (e.g., sticky queue handling) impacting throughput and workflow progress.
- Mutable State Insights (stale state)
  - Detects rare situations where workflow mutable state becomes outdated/stale, which can lead to workflow execution problems. Early detection helps avoid workflow stuck/duplication scenarios.
- Shard Controller (acquisition count/latency, item counts, closed shards)
  - Observes sharding mechanics for scaling and failover. Watching shard acquisition rates and latency is critical when diagnosing cold starts, uneven load, or partitioning-related outages.
- Persistence (requests/errors, by operation, latency)
  - Tracks interactions and error patterns with the persistence layer for all state and task operations. High latency or error rates may indicate DB issues, misconfiguration, or underlying infra problems that impact workflow state durability and progress.
- Kafka Client (requests, errors, publish latency)
  - Monitors the health of message-passing interactions (e.g., cross-cluster replication or event publishing). High error or publish latency can result in missed events or cross-service delays, crucial for debugging distributed Temporal setups.

This dashboard is essential for operators and SREs looking to maintain workflow execution reliability, detect bottlenecks or resource saturation, and perform root-cause analysis for both transient incidents and systemic issues. Detailed per-operation and per-component panels accelerate the diagnosis of workflow stuckness, cache inefficiency, transfer/timer backlogs, or persistence failures—enabling more proactive scaling, tuning, and incident response.



## Matching Service Dashboard

This dashboard delivers comprehensive observability for the Temporal Matching service, which is responsible for routing workflow and activity tasks to appropriate workers via task queues. It enables SREs and operators to assess task scheduling effectiveness, identify bottlenecks or overload scenarios in task routing, and catch issues in service-to-service dependencies or persistent storage interactions.

- Matching Requests Per Operation / Errors / Latency
  - Tracks API call volume, types, and errors for the Matching service. This helps detect spikes, new error modes, or slowdowns in task schedule/poll/ack operations, critical for maintaining workflow and activity progress.
- Tasks (polling, throttling, sync/async match latencies, forwarded calls)
  - Monitors polling rates on task queues (how frequently workers are requesting tasks), the occurrence of request throttling (which signals overload or rate limiting), and the latency associated with different task matching types (sync vs. async).
  - Forwarded call panels differentiate between local and remote task matches, highlighting inefficiencies or cross-cluster routing bottlenecks that can cause increased delivery latency or stuck workflows.
- Tasklist Manager (lease requests, failures, expired tasks, throttling, error breakdown)
  - Focuses on the internals of task queue management: lease requests show how task queue ownership is managed, expired tasks reveal dropped or delayed work, and throttling helps identify unbalanced load or scaling trouble.
  - The error breakdown gives insight into which kinds of task queue operations are failing most and why—supporting rapid diagnosis of capacity or configuration issues.
- Persistence (requests, errors, by operation, latency)
  - Tracks read/write volume, error patterns, and response times from the persistent storage layer. High or growing latency here may cause systemic lag in task scheduling and delivery, especially under heavy workloads.
- History Client (requests, errors, by operation, latency)
  - Observes the Matching service’s interactions with the History service. Requests per operation and latency analysis surface points where state fetches or updates become a bottleneck, often correlating with spikes in task queue lag or scheduling failures.

This dashboard is essential for anyone responsible for keeping Temporal task queues healthy and performant. It enables proactive detection of overload, task processing lag, or coordination failures between components, thereby supporting rapid scaling adjustments, incident triage, and smooth workflow and activity execution at scale.



## Visibility Dashboard

This dashboard is focused on monitoring Temporal's visibility subsystem—the part responsible for indexing, searching, and retrieving workflow execution records—and its underlying persistence layers. It enables SREs and platform engineers to track visibility task processing health, error rates, latency, and storage backend performance, whether using the standard database, advanced persistence (such as ElasticSearch), or both.

- Task Processor Visibility (Requests vs Errors, Error Breakdown, By Operation, Latency Statistics)
  - Monitors how visibility tasks (such as indexing workflow status or updating search attributes) are processed. Requests versus errors helps you quickly spot ingestion or indexing failures, while per-operation and error breakdowns enable precise root cause analysis.
  - Latency panels (average, p50, p95 for processing, overall, and end-to-end) help identify performance regressions and ensure visibility updates aren't introducing delays to workflow searchability.
- Standard Persistence (DB-backed visibility) Panels
  - Tracks requests, errors, operation breakdowns, and latency statistics for visibility tasks when using standard SQL persistence. These panels are crucial for recognizing misconfigurations, database slowdowns, or schema bottlenecks affecting workflow search and listing APIs.
  - General request vs error, errors by type, latency distribution, and per-operation metrics make it easy to isolate which visibility actions are most problematic or slow.
- Advanced Persistence (ElasticSearch) Panels
  - Mirrors the above metrics for deployments utilizing ElasticSearch for visibility. This includes requests, errors, operation breakdowns, and detailed latency tracking.
  - Since ElasticSearch can introduce new failure or performance modes (e.g., cluster unavailability, mapping errors), the advanced panels help detect index bloat, mapping issues, or inefficient queries.
  - p95 and average values for bulk processor stats reveal backlogs or stress under high write/read throughput.
- ElasticSearch Bulk Processor
  - Focuses on the ingestion path when using ElasticSearch: request vs error count, maximum bulk size, queued requests, and high-percentile write latencies. These metrics are key to catching downstream ES bottlenecks or replication lag.
  - p95 latency stats (WaitAdd, WaitStart, Commit) help you pinpoint where slowdowns occur in the ES write pipeline.
- ElasticSearch Miscellaneous (Doc Errors, Search Attribute Addition)
  - Tracks parsing/generation errors for indexed documents and the health of search attribute updates per workflow. This helps detect unexpected changes in workflow schema or faulty search attribute propagation.

This dashboard is indispensable for ongoing visibility, search, and workflow listing health in Temporal. By surfacing task queue delays, failed visibility updates, and persistence/ElasticSearch backpressure, operators can quickly respond to impaired workflow search, reporting, or listing functionality—whether due to operator error, scaling thresholds, or external system outages. It is crucial for maintaining SLAs around workflow observability and for diagnosing "missing" workflows or search regressions in production environments.

## Worker Service Dashboard

This dashboard provides targeted observability into the Temporal Worker service and its internal background processors (“scavengers”), the batch API processor, the parent close policy processor, and the replication pipeline. It is designed for SREs and operations teams to monitor background maintenance tasks, failure counts, throughput, and error conditions that can impact Temporal's self-healing, data retention, and cross-cluster replication.

- General (Scavenger Metrics: running/completed/dropped/failed/deferred tasks)
  - Tracks the lifecycle of scavenger (background cleanup) tasks. Sudden increases in dropped or failed tasks could signal issues with resource shortages, bugs in maintenance logic, or cluster-level trouble affecting cleanup or compaction.
- Executions Scavenger (Validation Requests, Errors, Executions Left to Scan)
  - Provides insight into the validation and cleanup of workflow executions, allowing operators to identify validation bottlenecks, error surges, or a backlog of executions remaining to be scanned and cleaned up.
- Taskqueue Scavenger (Outstanding, Processed, Deleted Tasks)
  - Reports on the health and progress of the task queue cleanup subsystem. Persistent outstanding tasks or drops in processed tasks may suggest scheduling problems or infrastructure issues, while spikes in deleted tasks indicate bursts of cleanup activity.
- History Scavenger (Successes, Skips, Errors)
  - Observes success, skipped, or failed attempts in history record cleanup, which is essential to healthy DB size and preventing data bloat. Growing errors or skips often require further investigation into schema or API contract changes.
- Search Attributes (Add Search Attributes Failure Count)
  - Tracks failed attempts to add custom search attributes, pointing to cluster configuration, permission, or schema management problems that could limit workflow observability or advanced filtering.
- Batcher (Requests, Errors)
  - Monitors incoming batch requests and associated error rates for the Temporal batch API. Elevated errors or drops in request processing may indicate scaling limits, coding bugs, or job queueing trouble affecting bulk workflow operations.
- Parent Close Policy Processor (Requests, Errors)
  - Surfaces activity related to automated parent-close policy execution, making it easier to spot surge failures or stuck requests that could leave dependent workflows open unintentionally.
- Replicator (DLQ enqueue requests/fails, messages, drops, errors)
  - Provides deep visibility into cross-cluster replication traffic and DLQ (dead-letter queue) backlog. High message drops, DLQ enqueue failures, or rising error rates could jeopardize multi-cluster consistency and require urgent operator attention.

This dashboard is crucial for monitoring Temporal’s critical background processing “hygiene” tasks and advanced features. It allows operators to quickly detect and respond to failures that, if left unchecked, would risk data bloat, retention policy violations, impaired workflow search, or replication gaps—ultimately safeguarding overall cluster health and availability.


## Exercise Conclusion

In this exercise, you explored a comprehensive suite of Temporal monitoring dashboards—each purpose-built to illuminate a different facet of system operation, performance, and reliability. By walking through these dashboards, you gained insights into core Temporal services, background maintenance, workflow processing, resource consumption, and the health of critical dependencies such as persistence storage and ElasticSearch.

**Key takeaways:**

- **Cluster Resource Awareness:** The Cluster Monitoring for Kubernetes dashboard ensures you can quickly detect and address infrastructure-level resource constraints before they escalate into service disruptions.
- **Service Deep-Dives:** Dashboards for Frontend, History, Matching, and Worker services expose request rates, error distributions, and latency profiles—empowering you to pinpoint which components are healthy, which are strained, and where troubleshooting efforts should begin.
- **Visibility and Search Readiness:** The Visibility Dashboard clarifies whether workflow execution records are indexed, searchable, and available to end users, as well as revealing backlogs or errors in the underlying storage mechanisms.
- **Operational Hygiene:** Worker Service dashboards give confidence in Temporal’s self-maintenance capabilities, ensuring tasks like clean-up, batched changes, and replication are functioning correctly and efficiently.

**Why This Matters:**
Regularly reviewing these dashboards enables operators, SREs, and developers to maintain optimal health of the Temporal platform. Quick detection and remediation of issues—whether at the infrastructure, service, or workflow layer—directly contribute to higher reliability, less downtime, faster incident response, and smoother scaling as workloads and teams grow.

This concludes the out-of-the-box metrics Temporal recommends, next let's look into what [additional custom metrics](./4.2.3.Additional-metrics.md) you should have.