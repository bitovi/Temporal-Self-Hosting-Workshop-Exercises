# 4.2: Visualizing Temporal metrics and traces
## Temporal Server Metrics


This **Temporal Server Metrics** dashboard is useful for getting the health of the Temporal service at a glance and can be used to begin isolating problems affecting Temporal services.

Within we have 5 groupings:
1. [Temporal](./4.2.1.Temporal-Server-Metrics.md#temporal)
2. [Workflow Completetion stats](./4.2.1.Temporal-Server-Metrics.md#workflow-completion-stats)
3. [Overview - All](./4.2.1.Temporal-Server-Metrics.md#overview---all)
4. [Persistence - All](./4.2.1.Temporal-Server-Metrics.md#persistence---all)
5. [Server Client - All](./4.2.1.Temporal-Server-Metrics.md#service-client---all)

## Temporal
### Service Availability (Based on Frontend Calls)
**Service Availability (Based on Frontend Calls)** monitors the error rate from frontend service calls in Temporal, presenting the calculated service availability as a percentage over time.

_Interpreting the Data_

1. The panel uses a time series graph to track the service's perceived availability.
    - Availability is calculated as:  
      100 - (errors / total requests * 100)
    - `service_errors` counts the number of failed frontend calls.
    - `service_requests` counts the total frontend calls.
    - The Prometheus `rate` function smooths these metrics over a 2-minute window for both errors and requests.
    - The OR condition with `vector(0)` ensures the query returns 0 if there are no errors.
    - The calculation gives the percentage of successful (i.e., non-error) requests over time.

_Correlation and Usefulness_
This panel would be used to monitor how often frontend service calls are failing and to quickly identify drops in availability. If there is a sudden reduction in availability, this serves as an early warning for operational issues, allowing operators to begin troubleshooting by looking for correlated spikes in errors or degraded dependencies within the Temporal service layer or connected infrastructure.


### Persistence Availability
**Persistence Availability** measures the success rate of persistence layer operations in Temporal (such as reading/writing to the database) by calculating the percentage of successful operations over time.

_Interpreting the Data_

1. The panel displays a time series graph representing persistence layer availability as a percentage.
    - Availability is calculated as:  
      100 - (persistence_errors / persistence_requests * 100)
    - `persistence_errors` counts failed persistence operations within a 2-minute rate window.
    - `persistence_requests` counts all persistence operations in the same window.
    - The OR with `vector(0)` ensures the query handles periods with zero errors gracefully.
    - The calculation output is the percentage of persistence requests that did *not* fail, shown over time.

_Correlation and Usefulness_
This panel is valuable for detecting problems with the storage backend (database, key-value store, or other persistence providers). A drop in persistence availability often correlates with outages, degraded workflow execution, or intermittent latency. By monitoring this, operators can quickly differentiate between service-level issues and persistence/backend failures, helping to zero in on the layer experiencing trouble.



### Shard Rebalancing
**Shard Rebalancing** visualizes the frequency of Temporal cluster rebalancing activities, including membership changes and shard item events, as time series data.

_Interpreting the Data_

1. The panel displays four key metrics over time, each plotted as its own time series:
    - `membership_changed`: 
        - Shows the rate of changes to the cluster membership (e.g., nodes joining or leaving) using `membership_changed_count`.
    - `shard_item_created`: 
        - Tracks the rate at which new shard items are created with `sharditem_created_count`, indicating rebalancing or new workflow workload.
    - `shard_item_removed`: 
        - Measures the rate at which shards are removed via `sharditem_removed_count`, which can happen due to scaling or failure recovery.
    - `shard_closed`: 
        - Plots how often shards are closed (`shard_closed_count`), a common occurrence during rebalancing or node failures.

2. Each of these is calculated using Prometheus’s `rate` function with a 2-minute window and aggregated using `sum` across all sources.

_Correlation and Usefulness_
This panel helps operators monitor the dynamics of cluster scalability and stability. Spikes in these metrics may indicate resharding operations, node churn, or recovery scenarios. Correlating shard rebalancing activity with performance or error panels can quickly identify if frequent rebalancing is impacting workflow latency, availability, or causing service instability—enabling root cause analysis for scaling or reliability challenges in Temporal.

### Shard Distribution
**Shard Distribution** provides real-time visibility into how Temporal shards are allocated across history service components in the cluster, displaying the number of shards owned by each relevant instance.

_Interpreting the Data_

1. The panel visualizes the current value of `numshards_gauge` for various history service instances as a time series.
    - `numshards_gauge` represents the current count of shards owned by a specific history node.
    - Multiple data series are included, each filtered by instance, node, pod, and other identifying labels.
    - The detailed label set allows operators to see:
        - The distribution of shards per instance (e.g., per pod or node)
        - How shard assignments change over time due to rebalancing, failover, or scaling events.

2. Useful observations:
    - Imbalances may reveal an uneven load across worker nodes.
    - Sudden drops or spikes can indicate rebalancing activity or node failures.

_Correlation and Usefulness_
This panel helps operators understand load distribution and resiliency in the Temporal cluster’s history service. It is especially useful during scaling events or after failures, as it allows correlation of shard moves with changes in processing rates, latency, or failure patterns. Operators can use it to fine-tune node allocation and swiftly diagnose issues related to shard ownership imbalances that may impact workflow processing.


## Workflow Completion Stats
TODO

## Overview - All

### Service RequestsVs Errors ($Service)
**Service RequestsVs Errors ($Service)** compares the rate of total service requests to the rate of errors for a specific Temporal service over time, providing direct visibility into the relationship between workload and failures.

_Interpreting the Data_

1. The panel displays two time series:
    - `requests`:
        - Calculates the sum of the rate of `service_requests` over a 5-minute window, showing how many requests the service receives.
    - `errors`:
        - Sums the rate of `service_errors` over the same window, reflecting the number of failed requests or errors.
2. The `$Service` variable allows this panel to be reused for any Temporal service, making it dynamic and context-aware.
3. Comparing these two series over time helps visualize patterns such as:
    - Spikes in errors that correspond to changes in request rate.
    - Periods where errors increase despite steady or low request rates (potential indicator of systemic issues or failures).

_Correlation and Usefulness_
This panel is particularly helpful for identifying trends between workload and error rates in any given Temporal service. Operators can quickly spot whether rising errors are associated with increased traffic or if errors persist independent of load—guiding focused troubleshooting on performance bottlenecks, capacity limits, or configuration regressions. It’s an essential first responder panel for health and incident monitoring.



### Service Requests Per Operation ($Service)
**Service Requests Per Operation ($Service)** charts the rate of requests broken down by specific operation types for a chosen Temporal service, showcasing which service operations are most active over time.

_Interpreting the Data_

1. The panel displays a time series for each operation by calculating:
    - `sum by (operation) (rate(service_requests[5m]))`
        - Sums the request rate for each type of operation over a 5-minute window.
2. Tracked operations include:
    - `DescribeNamespace`: Queries namespace metadata or configuration.
    - `GetSystemInfo`: Requests detailed service/system information.
    - `PollActivityTaskQueue`: Polls for available activity tasks in the queue.
    - `PollWorkflowTaskQueue`: Polls for workflow tasks, driving workflow execution.
    - `StartWorkflowExecution`: Initiates new workflow runs.
3. The `$Service` variable allows this panel to be filtered for any Temporal service, ensuring targeted and relevant visibility.
4. Patterns to look for:
    - Which operations are receiving the most traffic.
    - Spikes or dips in specific operations, which may indicate client usage patterns or issues.

_Correlation and Usefulness_
Operators use this panel to profile the usage and workload breakdown of any Temporal service at an operation level. It's helpful for identifying operational hotspots, unusual patterns (e.g., sudden drop in `PollWorkflowTaskQueue`), or performance issues related to specific API calls. This helps correlate client-side issues to backend behavior, optimize application usage, and tune Temporal deployment for real-world workloads.


### Service Errors Break Down ($Service)
**Service Errors Break Down ($Service)** provides a categorized, time-based view of common error types encountered by a specified Temporal service, tracking their rates to help diagnose and understand recurring failure modes.

_Interpreting the Data_

1. The panel displays several time series, each representing a distinct error type over time:
    - `Entity Not Found`: 
        - Tracks the rate of `service_errors_entity_not_found` (typically indicates missing workflows, activities, or other resources).
    - `Execution Already Started`: 
        - Captures the rate of `service_errors_execution_already_started` (occurs when a duplicate attempt is made to start a workflow that already exists).
    - `Resource Exhausted`: 
        - Measures `service_errors_resource_exhausted`, signaling quota or resource limits have been exceeded (could be rate limits, capacity, or system limits).
    - There is also a catch-all:
        - `sum by (__name__) (rate({__name__=~"service_errors_.*"}[5m]))` aggregates all error types that match the naming pattern.
2. Each metric uses Prometheus’s `rate` over a 5-minute window, with `OR on() vector(0)` ensuring a zero value when errors are absent.
3. The `$Service` variable lets you filter for the specific Temporal service in focus.

_Correlation and Usefulness_
Operators use this panel to pinpoint which error conditions are most prevalent, trace spikes in particular error types, and correlate with operational incidents. It’s essential for resolving client-impacting issues quickly—if “Resource Exhausted” is high during traffic spikes, it may be time to adjust quotas or performance tuning; frequent “Entity Not Found” errors may indicate referencing bugs or user errors. The panel accelerates root-cause isolation by showing which error categories are actively impacting service health.


### Service Errors By Operation ($Service)
**Service Errors By Operation ($Service)** tracks the error rate for specific operations on a Temporal service, allowing visibility into which API actions are experiencing the most issues.

_Interpreting the Data_

1. The panel shows a separate time series for each operation, presenting the sum of error rates per operation over a 5-minute window:
    - `sum by (operation) (rate(service_errors[5m]))`
        - Aggregates errors by the type of operation involved.
2. Highlighted operations include:
    - `{operation="PollActivityTaskQueue"}`: Errors while polling for activity tasks, possibly indicating worker unavailability or misconfiguration.
    - `{operation="PollWorkflowTaskQueue"}`: Errors occurring when polling the workflow task queue, which may reflect workflow worker connectivity or system bottlenecks.
    - `{operation="StartWorkflowExecution"}`: Errors starting workflows, pointing to issues like duplicate workflow IDs, missing permissions, or validation failures.
3. The `$Service` variable ensures the panel is specific to the chosen Temporal service, enabling flexible troubleshooting.
4. This breakdown can surface:
    - Which operations are most error-prone.
    - Whether error rates for a specific operation correlate with broader system issues or isolated user/API patterns.

_Correlation and Usefulness_
This panel pinpoints operational hotspots—if one operation type shows persistent or spiking errors, operators know where to focus investigation (e.g., worker scaling for polling errors, application logic for workflow startups). Combined with other workload and error panels, it provides granular context to recover from incidents, improve service quality, and diagnose end-user impact at the API level.


## Persistence - All

### Persistence Requests Vs Errors ($Service)
**Persistence Requests Vs Errors ($Service)** compares the total persistence layer operation requests to the number of persistence errors over time for a selected Temporal service, making trends and issues in backend persistence immediately visible.

_Interpreting the Data_

1. The panel presents two key time series:
    - `requests`:
        - Calculated as the sum of the rate of `persistence_requests` over a 5-minute window, indicating the volume of persistence layer operations (reads/writes, etc.).
    - `errors`:
        - The sum of the rate of `persistence_errors` over a 5-minute window, showing how often persistence operations fail.
    - Both metrics use the `OR on() vector(0)` pattern to ensure the query gracefully returns zero if data is missing.
2. The `$Service` variable scopes the panel to a particular Temporal service, so users can drill down on the persistence health specific to that part of the system.
3. You can quickly see:
    - If errors occur in bursts or in step with increases in persistence requests.
    - Baseline error rates versus normal operating workloads.

_Correlation and Usefulness_
Operators rely on this panel to correlate spikes in persistence layer errors with incoming persistence operation workload across Temporal services. It’s vital for distinguishing between high request volume (which may be healthy) and problematic backend errors (such as unresponsive databases or exhausted quota). Swiftly identifies persistence bottlenecks, uncovers infrastructure faults, and supports effective incident response and performance tuning.



### Persistence Requests Per Operation ($Service)
**Persistence Requests Per Operation ($Service)** breaks down the rate of persistence layer requests by specific operation, enabling detailed insight into how different types of storage actions are distributed and which are most active for the selected Temporal service.

_Interpreting the Data_

1. The panel displays a time series for each persistence operation by calculating:
    - `sum by (operation) (rate(persistence_requests[5m]))`
        - Aggregates the rate of persistence requests for each operation over a 5-minute interval.
2. The operations tracked include a comprehensive set of Temporal persistence actions, such as:
    - `AssertShardOwnership`, `CreateWorkflowExecution`, `GetClusterMembers`, `GetNamespace`, etc.
    - Ranging across actions like reading data, initializing, updating, listing, and handling various task types (transfer, timer, visibility, outbound, replication, etc.).
    - Covers metadata-related queries and mutations (`GetClusterMetadata`, `ListClusterMetadata`, `SaveClusterMetadata`, etc.), as well as operations related to shard management and cluster membership.
3. The use of the `$Service` variable makes this panel flexible for analyzing any Temporal service's backend persistence usage.
4. By observing the trend lines:
    - You can determine which operations dominate the backend workload.
    - Spot unusual increases, decreases, or patterns associated with specific data flows.

_Correlation and Usefulness_
This panel is vital for performance analysis, bottleneck identification, and operational troubleshooting. It helps operators trace high-latency or high-error operations back to their root activities, optimize database schema and storage strategies, and confirm whether the persistence layer is efficiently supporting active workflows and system events. It supports workload-aware scaling and highlights the persistence impact of various operational behaviors within Temporal.


### Persistence Errors By Operation ($Service)
**Persistence Errors By Operation ($Service)** provides a detailed breakdown of error rates for specific persistence operations in a selected Temporal service, highlighting where backend failures or issues are most concentrated.

_Interpreting the Data_

1. The panel shows a time series for each persistence operation’s error rate, calculated as:
    - `sum by (operation) (rate(persistence_errors[5m]))`
        - Aggregates errors by the type of persistence action over a 5-minute window.
    - Each time series corresponds to a specific operation, including:
        - `{operation="GetOrCreateShard"}`
        - `{operation="GetOutboundTasks"}`
        - `{operation="GetTimerTasks"}`
        - `{operation="GetTransferTasks"}`
        - `{operation="GetVisibilityTasks"}`
        - `{operation="ListClusterMetadata"}`
        - `{operation="ListNamespaces"}`
        - `{operation="RangeCompleteTimerTasks"}`
        - `{operation="UpdateShard"}`
        - `{operation="UpsertClusterMembership"}`
2. The `$Service` variable filters the panel to the chosen Temporal service, supporting granular, service-specific troubleshooting.
3. Observations to make:
    - Determine which backend operations are most error-prone.
    - Identify if error spikes align with certain workflows, cluster events, or user actions.
    - Track persistent issues with key data handling or shard management parts of the system.

_Correlation and Usefulness_
Operators use this panel to precisely locate problematic persistence actions that may be degrading performance, causing outages, or otherwise impacting Temporal’s reliability. By focusing on error-prone operations (like `UpdateShard` or `GetOrCreateShard`), teams can prioritize debugging and mitigation, refine database schemas or indexes, and align remediation efforts with real workload patterns to enhance backend health and resilience.

## Service Client - All

### Requests Vs Errors ($Service -> $Client)
**Requests Vs Errors ($Service -> $Client)** tracks the total client request rate versus the client error rate for a given Temporal service and client pair, providing visibility into client interaction quality and health.

_Interpreting the Data_

1. The panel displays two time series:
    - `requests`:
        - Sum of the rate of `client_requests` over a 5-minute window, showing how many requests clients are sending to the specified service.
    - `errors`:
        - Sum of the rate of `client_errors` over the same window, representing how many of those requests resulted in errors from the client’s perspective.
2. The variables `$Service` and `$Client` allow for dynamic selection, letting users analyze traffic and error patterns between any Temporal service and its individual clients.
3. Typical patterns to watch for:
    - Spikes in error rate relative to request rate (could indicate outages, timeouts, or misconfiguration).
    - Correlation of changes in request traffic with error trends (e.g., a surge of traffic leading to more errors).

_Correlation and Usefulness_
This panel is instrumental for client-facing troubleshooting. If a client’s error rate rises independently of overall request volume—or if error spikes align perfectly with traffic surges—it can quickly guide operators to problems in client implementation, connectivity, resource consumption, or upstream service health. It also helps confirm whether issues are broad or isolated to a single client-service relationship, streamlining incident response.

### Client Requests Per Operation ($Service -> $Client)
**Client Requests Per Operation ($Service -> $Client)** visualizes the request rate from a client to a Temporal service, broken down by each client operation, offering detailed insight into what activities clients are performing.

_Interpreting the Data_

1. The panel charts a time series for each recognized client operation using:
    - `sum by (operation) (rate(client_requests[5m]))`
        - Aggregates the 5-minute rate of requests per client operation.
2. Operations being tracked include (but are not limited to):
    - `HistoryClientStartWorkflowExecution`: Client requests to start workflow executions via the history service.
    - `MatchingClientCancelOutstandingPoll`: Client cancels an outstanding poll for a task.
    - `MatchingClientListNexusEndpoints`: Client requests a list of Nexus endpoints.
    - `MatchingClientPollActivityTaskQueue`: Client polls for activity tasks from the matching service.
    - `MatchingClientPollWorkflowTaskQueue`: Client polls for workflow tasks from the matching service.
3. The variables `$Service` and `$Client` enable dynamic focus, so you can profile behavior for any combination of Temporal service and client.
4. Insights:
    - Learn which client operations are most frequent.
    - Spot surges, anomalies, or lulls in particular client activities.
    - Identify baseline client usage patterns versus incident or failure activity.

_Correlation and Usefulness_
This panel is essential for understanding how individual clients interact with Temporal, which helps operators and developers correlate client activity with backend workload, track adoption of particular patterns or APIs, and diagnose application-specific or client-library issues. It also helps identify under- or over-utilization of Temporal features by specific clients, supporting optimization of workflow design and client implementation.


### Errors By Operation ($Service -> $Client)
**Errors By Operation ($Service -> $Client)** breaks down the error rate for client-initiated operations, showing which specific requests from a client to a Temporal service are resulting in failures over time.

_Interpreting the Data_

1. The panel charts a time series for each tracked client operation using:
    - `sum by (operation) (rate(client_errors[5m]))`
        - Aggregates the 5-minute rate of client errors by operation.
2. Operations covered include:
    - `HistoryClientStartWorkflowExecution`: Errors encountered by the client when trying to start workflows via the history service.
    - `MatchingClientListNexusEndpoints`: Errors in listing Nexus endpoints through the matching service.
    - `MatchingClientPollActivityTaskQueue`: Errors when polling activity task queues.
    - `MatchingClientPollWorkflowTaskQueue`: Errors while polling workflow task queues.
3. The panel uses `$Service` and `$Client` variables to dynamically select a Temporal service and client pairing, supporting detailed, contextualized troubleshooting.
4. Interpreting:
    - Patterns of increased errors in specific client operations may highlight integration bugs, configuration issues, or Temporal backend disruptions.
    - Comparing time series helps differentiate isolated client bugs from systemic issues.

_Correlation and Usefulness_
This panel is critical for zeroing in on client-driven failures. By breaking down which operations are most error-prone, operators and developers can rapidly identify problematic client code, misalignment with Temporal API best practices, or connectivity issues. Correlating error spikes here with request patterns or backend performance provides deep end-to-end troubleshooting power, enhancing reliability and user experience.


### Client Latency ($Service -> $Client)
**Client Latency ($Service -> $Client)** shows the high-percentile (95th) latency experienced by different Temporal service clients during specific operations. Its intent is to help operators identify potential bottlenecks or atypical delays in service interactions.

_Interpreting the Data_

1. The panel plots the 95th percentile latency using the `histogram_quantile(0.95, ...)` function.
    - This provides insight into the worst latency outliers for client calls, indicating performance experienced by nearly all users except the highest 5%.
2. The metric `client_latency_bucket` is measured over the past 5 minutes (`rate(client_latency_bucket[5m])`), ensuring recent performance trends are displayed.
    - This is summed and grouped by the `operation` label and histogram bucket edge (`le`), allowing the quantile calculation per client operation type.
3. The listed operations specify which client interactions are tracked:
    - `HistoryClientStartWorkflowExecution` — Time it takes for clients to initiate a new workflow.
    - `MatchingClientCancelOutstandingPoll` — Time taken to cancel outstanding poll requests.
    - `MatchingClientListNexusEndpoints` — Latency for requests to list Nexus endpoints.
    - `MatchingClientPollActivityTaskQueue` and `MatchingClientPollWorkflowTaskQueue` — Durations of polling for activity and workflow task queues.
4. On the chart, each line reflects latency for a distinct operation, providing per-operation breakdowns.
    - Spikes or sustained increases in a specific operation’s latency can highlight emerging issues.
    - Comparisons between operations help pinpoint problematic service boundaries or workloads.

This panel is useful for getting the health of the Temporal service at a glance and can be used to begin isolating problems affecting Temporal services.

_Correlation and Usefulness_
Operators/SREs use this panel to detect and investigate latency regressions or spikes in internal Temporal client communication. For example, if `HistoryClientStartWorkflowExecution` latency rises, it may suggest issues with workflow initiation or backend resource saturation. This visualization aids in correlating client latency trends with service incidents, deployment changes, or downstream outages, and is a key input for troubleshooting service slowdowns or unresponsiveness—driving root-cause analysis and prioritizing remediation efforts


