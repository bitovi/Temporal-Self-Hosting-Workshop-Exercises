# 4.2: Visualizing Temporal metrics and traces

Now that we our within our Grafana UI, it's time to begin dissecting the out-of-the-box metrics, what they represent and how we can interpret them.

## Dashboards Explored

On the left panel, click "Dashboards", you should see the following preconfigured dashboards:

1. Cluster monitoring for Kubernetes
2. Frontend Service Dashboard
3. History Service Dashboards
4. Matching Service Dashboard
5. SDK Metrics
6. Temporal Server Metrics
7. Visibility
8. Worker Service Dashboard

Some of the panels won't have any values yet as we have yet to perform load simulation. We'll discuss what the panel is doing and how we can interpret it and come back to it in later exercises. 

> [!TIP]
> There are way to many panels across the dashboards to really dig into individually, in this section we will instead dig into a few important ones. Use the reference doc to read more detail about the panels that aren't covered here.

## Temporal Server Metrics
The **Temporal Server Metrics** dashboard is useful for getting the health of the Temporal service at a glance and can be used to begin isolating problems affecting Temporal services.

This should be the first place you look when you need to diagnose or evaluate, from here you can almost always identify what then needs to be narrowed in on through other dashboards, such as the service specific dashboards.

The Temporal Server Metrics Dashboard has 5 rows:
1. [Temporal](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#temporal)
2. [Workflow Completion stats](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#workflow-completion-stats)
3. [Overview - All](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#overview---all)
4. [Persistence - All](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#persistence---all)
5. [Server Client - All](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-client---all)


### Temporal
This is the top most overview of the Temporal Server, it provides time series graphs for overall availability, workflow/activity state and core components such as shard management.

There are more panels that can be further explored in this group such as workflow/activities which give you a view of overall workflow/activity state.

#### Important dashboards
1. [Serivce Availability](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-availability-based-on-frontend-calls)
1. [Persistence Availability](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#persistence-availability)
1. [Shard Rebalancing](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#shard-rebalancing)
1. [Shard Distribution](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#shard-distribution)

#### An example usage may look like
A support ticket has been raised in which a team is noticing a higher than normal time for workflow completion. The operation team member assigned begins the investigation by opening teh Grafana UI to the Temporal server metrics dashboard. Using the Temporal group he is able to see that the shard distribution is unstable for the last 30 minutes thus confirming the concern.


#### Service Availability (Based on Frontend Calls)
**Service Availability (Based on Frontend Calls)** monitors the error rate from frontend service calls in Temporal, presenting the calculated service availability as a percentage over time.

_Interpreting the Data_
The panel uses a time series graph to track the service's perceived availability.

_Correlation and Usefulness_
This panel would be used to monitor whether there have been service interuptions to the frontend service. If there is a sudden reduction in availability, this serves as an early warning for operational issues, allowing operators to begin troubleshooting by looking for correlated spikes in errors or degraded dependencies within the Temporal service layer or connected infrastructure.

#### Persistence Availability
**Persistence Availability** measures the success rate of persistence layer operations in Temporal (such as reading/writing to the database) by calculating the percentage of successful operations over time.

_Interpreting the Data_
The panel displays a time series graph representing persistence layer availability as a percentage.


_Correlation and Usefulness_
This panel is valuable for detecting problems with the storage backend (database, key-value store, or other persistence providers). A drop in persistence availability often correlates with outages, degraded workflow execution, or intermittent latency. By monitoring this, operators can quickly differentiate between service-level issues and persistence/backend failures, helping to zero in on the layer experiencing trouble.ß

#### Shard Rebalancing
**Shard Rebalancing** visualizes the frequency of Temporal cluster rebalancing activities, including membership changes and shard item events, as time series data.

_Interpreting the Data_

1. The panel displays four key metrics over time, each plotted as its own time series:
    - `membership_changed`: 
        - Shows the rate of changes to the cluster membership (e.g., nodes joining or leaving) using `membership_changed_count`.
    - `shard_item_created`: 
        - Tracks the rate at which new shard items are created with `sharditem_created_count`, indicating rebalancing or new workflow workload.
    - `shard_item_removed`: 
        - Measures the rate at which shards are removed via `sharditem_removed_count`, which can happen due to scaling or failure recovery.
    - `shard_closed`: 
        - Plots how often shards are closed (`shard_closed_count`), a common occurrence during rebalancing or node failures.

_Correlation and Usefulness_
This panel helps operators monitor the dynamics of cluster scalability and stability. Spikes in these metrics may indicate resharding operations, node churn, or recovery scenarios. Correlating shard rebalancing activity with performance or error panels can quickly identify if frequent rebalancing is impacting workflow latency, availability, or causing service instability—enabling root cause analysis for scaling or reliability challenges in Temporal.

#### Shard Distribution
**Shard Distribution** provides real-time visibility into how Temporal shards are allocated across history service components in the cluster, displaying the number of shards owned by each relevant instance.

_Interpreting the Data_
1. The panel visualizes the current value of `numshards_gauge` for various history service instances as a time series.
2. Useful observations:
    - Imbalances may reveal an uneven load across worker nodes.
    - Sudden drops or spikes can indicate rebalancing activity or node failures.

_Correlation and Usefulness_
This panel helps operators understand load distribution and resiliency in the Temporal cluster’s history service. It is especially useful during scaling events or after failures, as it allows correlation of shard moves with changes in processing rates, latency, or failure patterns. Operators can use it to fine-tune node allocation and swiftly diagnose issues related to shard ownership imbalances that may impact workflow processing.

---

## Workflow Completion Stats
The **Workflow Completion Stats** group narrows in on information about workflow and activity metrics such as successful/failed runs, current number of running workflows, etc. We would use this to identify problematic workflows or activities. For example it could be that a new workflow introduced hasn't been well tested and a percentage of their runs are failing.

### An example usage may look like
We've opened our Grafana UI to our Temporal Server metrics dashboard, while looking through the workflow and activities panel we notice that there are spikes of failing workflows. We want to dig further into this annomoly so we then open this group to better visualize those metrics and begin to narrow in on the problematic workflow / activity.


## Overview - All
The **Overview - All** group provides high-level insights into the individual services (Frontend, History, Matching & Worker) such as requests vs errors, number of operations, what operations are having errors, etc. We would use this to identify problems with individual services which we would then further dig into in using the service dashboards.

### Important dashboards
1. [Service RequestsVs Errors](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-requestsvs-errors-service)
1. [Service Requests Per Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-requests-per-operation-service)
1. [Service Errors Break Down](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-errors-break-down-service)
1. [Service Errors By Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#service-errors-by-operation-service)

### An example usage may look like
You opened the Grafana UI and navigated to the Temporal server metrics dashboard when you notice that the Service availability went from 100% to ~75%, concerned you begin to investigate by opening the Overview - All dashboard and notice that an abnormal amount of failures have started cropping up on the Frontend service.

## Persistence - All
The **Persistence - All** group provides high-level insights into the persistence layer in a similar way that **Overview - All** does with individual services.

### Important dashboards
1. [Persistence Requests Vs Errors](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#persistence-requests-vs-errors-service)
1. [Persistence Requests Per Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#persistence-requests-per-operation-service)
1. [Persistence Errors By Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#persistence-errors-by-operation-service)

### An example usage may look like
You've identified an issue with the shard component via the **Temporal panel group** > _Shard distribution panel_, something seems wrong so you've decided to dig into the issue further by opening the **Persistence - All panel** group. While reviewing the _Persistence Requests Per Operation_ panel you see an abnormal amount of `GetOrCreateShard` operations and you confirm the issue by then looking at the _Persistence Errors By Operation_ to see that the operation is creating an above normal amount of new shards.

## Service Client - All
The **Service Client - All** panel group is used to identify service operation issues, it provides similar information to the **Overview - All** but at a more detailed level. Additionally it allows operators to dig into service to service specific issues, such as communication between matching and frontend.

### Important dashboards
1. [Requests Vs Errors](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#requests-vs-errors-service---client)
1. [Client Requests Per Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#client-requests-per-operation-service---client)
1. [Errors By Operation](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#errors-by-operation-service---client)
1. [Client Latency](./4.2.1.Temporal-Server-Metrics.md_Referenec-Doc#client-latency-service---client)

### An example usage may look like
As an operator you've been tasked with creating a new namespace, while doing so your temporal CLI seems to hang and the operation takes multiple calls before finally you get a result back. Something doesn't feel right about this so you open the Grafana UI **Temporal server metrics** dashboard and look into the _Client Latency_ panel. You can see the operation is being called but has a latency of >1 minute, you also notice other operations had a sudden spike in latency for the frontend. You decide to investigate why the frontend might be having these issues.

## Exercise Conclusion

In this exercise, you explored the **Temporal Server Metrics Dashboard**, gaining experience with the fundamental panels and groupings that provide health and performance insights into the Temporal platform. By systematically reviewing these dashboard sections—including service availability, persistence health, shard allocation, request and error breakdowns, and client interaction metrics, you have built a concrete understanding of how to monitor and diagnose core Temporal operations.

**Key takeaways:**

- **Service & Persistence Visibility:** Panels tracking service and persistence availability empower you to catch drops in reliability early, surfacing operational issues before they escalate into incidents.
- **Scaling & Shard Dynamics:** Shard rebalancing and shard distribution visualizations reveal patterns around scaling activities, node churn, and the internal distribution of workload which is crucial for diagnosing capacity issues and cluster stability.
- **Workload & Error Profiling:** Breakdowns of requests, errors, and latency by both operation and service/client pairing help pinpoint trouble spots, reveal operational hotspots, and enable effective root cause analysis.
- **End-to-End Transparency:** By correlating trends across service, client, and persistence panels, operators can trace symptoms to root causes, whether in the application, Temporal’s core, or its infrastructure dependencies.

**Why This Matters:**
Regular use of the Temporal Server Metrics Dashboard allows operators and SREs to respond rapidly to emerging issues, maintain optimal platform reliability, and make informed decisions during scaling events or incident remediation.

This concludes the foundational metric panels Temporal recommends for core system observability. In the next exercise we will simulate pod failure and see how our dashboards respond and what it tells us.

Let's now get hands-on by [simulating failure and seeing how Temporal responds](./4.3.Simulating-failures-and-viewing-retry-and-alerting-behavior.md)
