# 4.2: Visualizing Temporal metrics and traces

Now that we our within our Grafana UI, it's time to begin dissecting the out-of-the-box metrics, what they represent and how we can interpret them.

## Dashboards Explored

On the left panel, click "Dashboards", you should see the following preconfigured dashboards:

1. Cluster monitoring for Kubernetes
2. Frontend Service Dashboard
3. History Service Dashboards
4. Matching Service Dashboard
5. SDK Metrics
6. Temporal Server Metrics
7. Visibility
8. Worker Service Dashboard

**Cluster monitoring for Kubernetes** is not in scope for this workshop so we'll skip it.

### Temporal Server Metrics
Let's start by opening the **Temporal Server Metrics** dashboard.

This dashboard is used to monitor the Temporal service overall health and performance. Within we have 5 groupings:
1. Temporal 
2. Workflow Completetion stats
3. Overview - All
4. Persistence - All
5. Server Client - All

Some of the panels won't have any values yet as we have yet to perform load simulation. We'll discuss what the panel is doing and how we can interpret it here and come back to it in later exercises. 

#### Temporal
**Service Availability** is measuring service availability by calculating the percentage of successful frontend calls to your service, over a moving window.

_Interpreting the Data_

- 100% value: Perfect availability; no errors in the last 2 minutes.
- <100% value: Some errors are happening. For example, if it drops to 98%, it means 2% of requests are (on average) returning errors.
- Sudden Drop: Indicates a spike in error rate and thus a potential incident or problem.
- Persistent Low Values: Chronic issues or outages.

This panel is useful for getting the health of the Temporal service at a glance and can be used to begin isolating problems effecting Temporal services.

---

**Persistence Availability** is a measure of the percentage of successful persistence-related operations (like writes, reads, or updates to a database, disk, or storage system) over time.

_Interpreting the Data_


- 100%: Your persistence layer is operating flawlessly (no errors in the last 2 minutes).

- Lower values (e.g., 99%): Indicates that 1% of persistence operations are currently failing.

- Dips/spikes: Correspond to real-time changes in the error rate. If you see a sudden drop, your storage/database may be experiencing an outage or degraded performance.

- Trends over time: Recurring dips could signal ongoing issues (e.g., during backup windows or high load).


This panel is useful for getting the health of the Temporal persistence layer at a glance and can be used to begin isolating problems effecting the Temporal persistence layer.

---

**Workflow Tasks** displays the rate at which various workflow-related operations (such as adding, starting, completing, failing tasks, and task timeouts) are occurring over time. Each query (A-F) tracks a specific event in the workflow task lifecycle, letting you observe task processing, completion, failures, and timeout trends on your system.

Interpreting the Data

1. **(A)** AddWorkflowTask — Tracks how frequently new workflow tasks are being added.
    - High values: Surge of new work (could indicate normal load or a spike).
    - Sudden drops: Likely a frontend issue or ingestion bottleneck.

2. **(B)** RecordWorkflowTaskStarted — Measures how often workflow tasks are started.
    - Lags behind A: Potential scheduling or resource starvation issues.
    - Near-zero: No tasks are being picked up for execution.

3. **(C)** RespondWorkflowTaskCompleted — How often workflow tasks complete successfully.
    - Should track with B: If not, some tasks aren’t completing.
    - If this drops but B is steady: Increased task failures/timeouts.

4. **(D)** RespondWorkflowTaskFailed — Counts failed workflow task completions.
    - Spikes: Indicate issues with correctness, dependencies, or resource exhaustion.
    - Persistent elevation: Chronic problem with workflow handlers or logic.

5. **(E)** schedule_to_start_timeout — How many tasks are timing out waiting to be picked up (never started on time).
    - Spikes/dips: Scheduler capacity problems, worker pool starvation, or backlog buildup.

6. **(F)** start_to_close_timeout — Number of tasks timing out during execution (started but not finished in time).
    - Spikes: Poor performance, bugs, or infrastructure slowness.

This panel is useful for tracking your workflow system’s health at each stage: you can spot bottlenecks (difference between add and started), reliability problems (increase in failed or timeouts), and overall throughput. Use this to correlate performance or outage incidents with workflow task flow and processing behavior.

---


**Activities** shows the rate at which activity tasks are added, started, and completed (either successfully, by failure, or by cancellation) over time. Each query (A-C) tracks a key phase in the activity task lifecycle, helping to monitor the flow and outcomes of individual activities within workflows.

_Interpreting the Data_

1. **(A)** AddActivityTask — Tracks how frequently new activity tasks are being added.
    - High values: Indicates increased workflow activity or input load.
    - Sudden drops: Could indicate upstream issues, such as workflow throttling or service input failures.

2. **(B)** RecordActivityTaskStarted — Measures how many activity tasks are started by workers.
    - Lags behind A: May indicate queue backing up, resource shortages, or slow worker response.
    - Zero or near-zero: No tasks are being picked up, possibly due to worker failure or disconnection.

3. **(C)** RespondActivityTaskCompleted | Failed | Canceled — Sums the rates of activities being completed, failed, or canceled, showing total task resolution (of any outcome).
    - Should generally track with B: Ideally, most started activities resolve (complete, fail, or cancel) promptly.
    - Decreasing trend: Less task resolution—could point to stuck/running activities, slower execution, or failing worker processes.
    - If "A" is steady but "C" drops: Activities are being enqueued but not getting finished; investigate worker capacity or bugs.

This panel is helpful for identifying bottlenecks in your activity processing pipeline, monitoring for activity build-up, and ensuring that tasks are being executed and resolved at an expected rate. By comparing the three rates, you can quickly spot where tasks might be stuck or dropped.

---


**Shard Rebalancing** tracks the rates of events involved in the rebalancing process of data or task shards across service nodes. Monitoring these metrics helps ensure healthy redistribution, addition, and removal of shards in your distributed system.

_Interpreting the Data_

1. **(A)** membership_changed_count — How often cluster membership changes (e.g., node joins, leaves, crashes) are occurring.
    - Spikes: Could indicate deployments, node failures, scaling events, or instability in your cluster.
    - Persistent elevation: Suggests ongoing instability or churn, which could impact system health or performance.

2. **(B)** sharditem_created_count — Rate of new shard items being created (i.e., new shards assigned or initialized).
    - Spikes: Likely after membership changes, such as auto-scaling events or after a failover.
    - Unusual increases: May imply excessive rebalancing or repeated shard redistributions.

3. **(C)** sharditem_removed_count — Rate of shard items being removed from nodes.
    - Regular baseline: Expected when rebalancing or decommissioning nodes.
    - Large spikes: Could mean aggressive rebalancing, node failures, or manual interventions.

4. **(D)** shard_closed_count — Number of times shards are being closed (released or no longer managed by a node).
    - Spikes: Often follow shard removal or node shutdown events.
    - Persistently high: May signal instability, excessive rebalancing, or configuration issues.

This panel is useful for detecting, diagnosing, and understanding the dynamics of shard movement and rebalancing in your cluster. Large or frequent changes across these metrics may indicate cluster instability, unhealthy nodes, or misconfiguration. Use this panel to correlate workload distribution issues or operational events (like outages or auto-scaling).

---


**Shard Distribution** displays the current number of shards assigned (likely per host, cluster, or another entity, depending on the metric labels) using the numshards_gauge metric. This panel helps visualize how shards are distributed across the system in real time.

_Interpreting the Data_

- Stable number per entity: Indicates a consistent shard distribution. If each node/host is responsible for a similar number of shards and the values are steady, balance is healthy.
- Sudden change (spike/dip): May indicate a scaling event, failover, rebalancing operation, or a node entering/leaving the cluster.
- Uneven distribution: If some hosts have significantly more or fewer shards, it may indicate imbalance, a stuck rebalance, or resource contention.
- Zero values: Could mean a node stopped reporting metrics, lost all its shards, or is unhealthy/faulted.

This panel is useful for early detection of shard imbalance problems, under-provisioned resources, or the need for manual intervention. Monitoring the shard distribution can help you optimize for load balancing, fault tolerance, and overall system health.

---

Let's now simulate failure and interpret the data. Within Grafana, on the top right, select "Last 5 minutes" for a time scale and "auto" for a refesh rate.

![alt text](5-min-and-auto-refresh.png "5 min scale and auto refresh")


Next open your terminal window/tab and perform the following:
```bash
$ kubectl get pods
# Copy the name of the "frontend" pod, it will look like: my-temporal-frontend-7bfdb64c85-cbfsp
$ kubectl delete pod/my-temporal-frontend-7bfdb64c85-cbfsp
```

Navigate back to Grafana and watch the **Service Availability** panel, in the next few moments we will see our availability drop from 100% to 0%. Kubernetes uses a Deployment that uses a replica-set so in the next few moments a new pod will be created and ready. We'll then see our panel jump from 0% back up to 100%. 


Next let's do the same with the Cassandra pods and see what happens to our **Persistence Availability** panel. Again notice the drop in availability %.

With that, let's close the Temporal group on the top left of the Grafana UI and move on.

Additionally, look at the **shard information** panels, notice how they _may_ have become unstable, this is a consequence of using a local Kubernetes cluster to host our Temporal service. For now we can ignore the instability and move on.


#### Workflow Completion Stats

All of this panel will be empty until we begin load simulation, but again, let's review the panels to understand what they are telling us.

**Workflow Completion Overview** shows the rates at which workflows are being completed, either by success, failure, timeout, termination, or cancellation. Each query (A–E) highlights a different type of workflow outcome, allowing you to monitor overall workflow health and diagnose completion trends over time.

_Interpreting the Data_

1. **(A)** workflow_success — Counts the rate of workflows completing successfully.
    - High/steady values: System is processing and finishing workflows as intended.
    - Drop: Indicates a slowdown, backlog, or possible systemic failure.
1. **(B)** workflow_failed — Tracks workflows completing due to a failure condition.
    - Spikes/increased rate: May indicate application bugs, unhandled edge cases, or infrastructure issues affecting reliability.
    - Persistent elevation: Chronic underlying issues need investigation.
1. **(C)** workflow_timeout — The rate of workflows that failed to complete within their deadline and timed out.
    - Spikes: Often a sign of downstream slowness, task queue backlogs, or resource exhaustion.
    - Elevated baseline: May reflect inefficient workflow designs or capacity issues.
1. **(D)** workflow_terminate — Workflows that were explicitly terminated before normal completion.
    - Increases: Usually tied to user/admin intervention, system errors, or automation scripts.
    - Consistent presence: Could indicate improper automation or unwanted disruption.
1. **(E)** workflow_cancel — Workflows canceled before completion (may be by user request or automation).
    - High values: Normal if cancellations are part of your business logic, but unexplained increases should be examined for user errors or misconfiguration.

This panel helps you quickly assess workflow system throughput and outcome quality. By tracking these metrics together, you can identify spikes in failures, timeouts, or cancellations and correlate these events with system changes or incidents, empowering you to investigate and resolve issues promptly.

---

The additional panels in this section isolate and further break down by status per namespace.


#### Overview - All

All of the panels here have the option to drill down on individual services. On the top of the overall dashboard select the _service_ dropdown and select the individual service that you're interested in drilling down into. In our small system there may be little change between them, but in larger systems the distinction between services will be more pronounced.

**Service RequestsVs Errors ($Service)** compares the rates of total service requests to error responses for the selected service over time. This panel visualizes both metrics together to help you quickly spot changes in overall load and error frequency.

_Interpreting the Data_

1. **(A)** service_requests — Total number of service requests received per second (smoothed over 5 minutes).
    - Rising values: Indicates increasing usage or load on the service.
    - Sudden drops: Could signal an upstream issue, outage, or reduced user activity.
    - Flatlining: No requests are being received—service might be down or disconnected.
1. **(B)** service_errors — Total number of error responses returned per second (smoothed over 5 minutes).
    - Low/steady values: Indicates stable operation and reliable service.
    - Spikes: May be caused by outages, bad deployments, configuration issues, or dependent service failures.
    - Persistent elevation: Ongoing problem affecting user experience or service reliability.

Requests and Errors together: Watching both lines together can help you correlate spikes in errors to increases in traffic, recent deployments, or infrastructure events. For healthy operations, errors should track close to zero, even as load (requests) varies.

This panel enables fast identification of error rate trends versus overall traffic, making it easier to detect reliability issues, abnormal request patterns, or the impact of outages on the selected service.

---

**Service Requests Per Operation ($Service)** breaks down the rate of service requests by each type of operation within the selected service. This panel shows how frequently each operation is being called over time, helping you identify usage patterns and hotspots.

_Interpreting the Data_

- Each line/series: Represents the request rate for a specific operation (e.g., “CreateUser”, “GetItem”, etc.).
- Stable trends: Expected if client behavior and traffic patterns are consistent.
- Spikes for one operation: May indicate a batch job, a bug, a DDoS attack, or a change in usage patterns.
- Operations at/near zero: Could indicate deprecated, infrequently used, or malfunctioning endpoints.
- Sudden appearance/disappearance: Could signal recent deployments (new operations added or removed) or outages affecting specific functionality.
- Comparing operations: Helps identify which endpoints are most heavily used, and alerts you to changes in traffic composition that might need attention.

This panel is useful for troubleshooting issues with individual operations, scaling resources according to usage, and understanding the core usage of your service features. It provides a granular view that complements broader request/ error metrics.

---

**Service Errors Break Down ($Service)** displays the rates of common error types returned by the selected service. This panel helps identify which error conditions are most frequent, exposing systemic or usage-related issues at a granular level.

_Interpreting the Data_

1. **(A)** service_errors_entity_not_found — Rate of “entity not found” errors (e.g., item/user/workflow did not exist).
    - Persistent presence: Could signal clients requesting deleted/missing resources or relying on stale data.
    - Spikes: May suggest recent data deletions, sync problems, or faulty client logic.
1. **(B)** service_errors_execution_already_started — Rate of errors from attempts to start an already-running execution/process.
    - Consistent rate: May be a normal application flow.
    - Sudden spikes: Possible client bugs, duplicate requests, or unhandled concurrency.
1. **(C)** service_errors_resource_exhausted — Rate of “resource exhausted” (quota limits hit, rate-limiting, or out-of-capacity) errors.
    - Spikes/increase: Indicates users or system are hitting resource limits, which could degrade service or require scaling/limit adjustments.
    - Ongoing elevation: May necessitate resource planning or wider architectural review.
1. **(D)** All service error types (sum by __name__ of service_errors_*) — Combined view: shows rates for each error category dynamically (not just A-C). Useful to detect emerging or less common error types.
    - New series appearing: May correspond to new problems, new error codes, or recent deployments.
    - Dominant series: Highlights the main issues affecting the service at that time.

This panel is valuable for rapid diagnosis of evolving problems, showing which errors spike first during an incident, and giving clues for prioritizing fixes based on error class prevalence. Use it to correlate client impacts, system bottlenecks, or traffic anomalies with specific error types.

---

**Service Errors By Operation ($Service)** presents the rate of error responses grouped by individual operation for the selected service. This panel lets you see which service actions—such as endpoints or RPC methods—are responsible for the most errors over time.

_Interpreting the Data_

- Each line/series: Represents error rate for a specific operation (e.g., PollActivityTaskQueue, PollWorkflowTaskQueue, etc.).
- Persistent high for an operation: Indicates an ongoing bug, integration issue, or misuse in that specific function.
- Spikes in one operation: Typically correspond to incidents, bad code deployments, or backend/resource failures affecting only part of the service.
- All lines spike: Suggests a broad incident impacting the entire service.
- Sudden appearance of a new operation with errors: Could indicate newly released code, new features, or a regression.
- Operations at/near zero: Typically healthy and stable; little or no errors being returned for those endpoints.

This panel is essential for quickly identifying which parts of your service need the most attention during troubleshooting, root cause analysis, or incident response. You can also use it to evaluate the impact of recent releases on reliability for each operation.

---

**Service Latency ($Service)** shows the 95th percentile (p95) response latency for each operation in the selected service. This panel highlights how long the slowest (but not most extreme) requests take to complete, broken down per operation, and is crucial for performance monitoring and user experience.

_Interpreting the Data_

- Each line/series: Represents the p95 latency (in milliseconds or seconds) for a specific operation (e.g., PollActivityTaskQueue, PollWorkflowTaskQueue, StartWorkflowExecution, etc.).
- Low/stable values: Indicate consistently fast operations—service is healthy from a latency perspective for most users.
- Rising trend: May suggest emerging performance bottlenecks, load increases, or downstream service slowness.
- Sudden spikes: Often correlate with performance regressions, dependency failures, network issues, or scaling events.
- Only some operations spike: Pinpoints which endpoints are experiencing degraded performance, helping to target investigations precisely.
- All operations spike: Likely a systemic problem, affecting the whole service or infrastructure.

This panel is valuable for tracking service performance, meeting SLO/SLA objectives, and identifying latency outliers before they significantly impact customers. Monitoring the p95 (rather than average) gives an early warning for “long tail” slow requests that can degrade user experience.

#### Persistence - All

**Persistence Requests Vs Errors ($Service)** compares the total rate of persistence (database/storage) requests to the rate of persistence errors for your service. This panel provides a broad view of backend storage activity and quickly highlights periods of abnormal failure.

_Interpreting the Data_

1.  **(A)** persistence_requests — Total number of persistence operations per second (smoothed over 5 minutes).
  - Higher values: Reflect increased backend workload—may result from normal user growth, spikes in traffic, or bulk operations.
  - Sudden drops: May indicate issues in upstream service logic, ingestion outages, or system downtime.
  - Flatlining: Could signal an outage, a severe bug, or that the persistence layer is completely unavailable.
1.  **(B)** persistence_errors — Total number of persistence failures per second (smoothed over 5 minutes).
  - Low/near zero: Indicates a healthy, reliable backend.
  - Spikes: Signal issues such as database unavailability, data contention, resource exhaustion, or logical errors in storage access.
  - Persistent elevation: Suggests chronic backend problems, possible misconfiguration, scaling limitations, or schema incompatibilities.

Requests and Errors together: Correlating spikes in errors with periods of increased activity can indicate backend overload, but high errors during normal or low request volume often points to operational incidents or code/core logic problems.

This panel is vital for quickly detecting backend reliability issues, supporting incident response, and helping engineering teams distinguish between normal load patterns and moments of storage system duress.

---

**Persistence Requests Per Operation ($Service)** shows the rate of persistence (database/storage) requests grouped by each specific operation in your service, giving visibility into how your system interacts with its persistent store(s) across a range of functionalities.

_Interpreting the Data_

- Each line/series: Represents the request rate (smoothed over 5 minutes) for a specific persistence operation, such as:
    - AssertShardOwnership, GetOrCreateShard, UpdateShard: Requests related to shard management, leader election, and partitioning.
        - High rates may correlate with resharding, failover, or scaling events. Sudden drops may indicate partition loss or leadership problems.
    - CreateWorkflowExecution, GetWorkflowExecution: Creating and fetching workflow execution data.
        - Frequent activity here reflects workflow churn; spikes may signal high user activity or batch processing.
    - Get/Range/List/UpsertClusterMetadata, SaveClusterMetadata, GetClusterMembers, PruneClusterMembership: Managing and querying cluster metadata and membership.
        - Patterns reflect cluster scaling, topology changes, or configuration sync events.
    - Get/Range/ListNamespaces, InitializeSystemNamespace: Interactions with application or platform namespaces.
        - Increased rates can occur during onboarding, migration, or listing-heavy administrative operations.
    - Get/List/Range/Complete/Prune/Transfer/Visibility/Outbound/Timer/ReplicationTasks: Operations involving task queues, replication logic, timer triggers, and workflow visibility.
        - High/surging rates may be due to increased workflow activity, backlogs, or traffic shifts; drops might point to stuck queues or worker outages.
    - ListNexusEndpoints: Querying endpoint registry; may spike on configuration reloads or health checks.
    - UpsertClusterMembership: Membership info updates, associated with cluster dynamics or failover.
- Comparing operations: Reveals which persistence actions are driving backend load or dominating storage traffic.
- Sudden spikes in any operation: May point to system migrations, backfills, deployment events, or unanticipated workload surges.
- Sudden drops or flatlining: Can indicate disabling of functionality, outages, or communication breakdowns.
- Baseline differences among operations: Help with capacity planning and prioritizing storage optimization.

This panel is critical for observing backend activity patterns, diagnosing bottlenecks in specific database workflows, and predicting potential issues before they impact service stability or performance.

---


**Persistence Errors By Operation ($Service)** displays the rate of errors encountered in the persistence (database/storage) layer, grouped by specific operation. Each operation represents a distinct type of database or storage request that your service is making. This panel helps pinpoint which storage interactions are failing most frequently and may require targeted investigation.

_Interpreting the Data_

- Each line/series: Is the error rate (per second, smoothed over 5 minutes) for a specific storage operation, such as:
    - GetOrCreateShard: Errors encountered when attempting to fetch or create a shard in storage.
        - Spikes may point to storage unavailability, data contention, or schema/metadata issues.
    - GetOutboundTasks: Errors during retrieval of outbound (scheduled or pending) tasks.
        - Elevations may signal queue corruption, internal mismatches, or missing task data.
    - GetTimerTasks, GetTransferTasks, GetVisibilityTasks: Errors fetching timer, transfer, or visibility task records from storage.
        - Problems here can indicate partition loss, schema migration faults, or resource constraints.
    - ListClusterMetadata & ListNamespaces: Errors listing global cluster metadata or registered namespaces.
        - Spikes may indicate wide storage outages, RPC query failures, or access privilege problems.
    - RangeCompleteTimerTasks: Errors completing or cleaning up ranges of timer tasks.
        - May suggest database lock contention, missing records, or transaction failures.
    - UpdateShard: Errors updating shard records (state, ownership, etc.).
        - Frequent issues can point to write conflicts, leader election churn, or storage saturation.
    - UpsertClusterMembership: Errors during upsert (insert/update) of cluster membership information.
        - Problems here may affect scaling, failover, or cluster health monitoring.
- Persistent errors in an operation: Indicates recurrent storage-level issues needing targeted investigation and resolution (could impact system stability or throughput).
- Sudden spikes: Often due to schema migrations, failovers, code deployments, or temporary backend outages.
- Multiple lines spike together: Suggests shared infrastructure problems or system-wide incidents affecting several storage functions.
- Operations with near-zero errors: Indicate those persistence flows are currently healthy.

This panel is crucial for root-cause analysis in storage or database-related incidents, surfacing the specific operations that are failing and enabling precise, data-driven troubleshooting within the persistence subsystem.

---


**Persistence Latency ($Service)** shows the 95th percentile (p95) latency for persistence (database/storage) operations, broken down by operation for the selected service. This panel focuses on how long the slowest (but not most extreme) storage operations are taking to complete, helping identify latency bottlenecks and performance regressions.

_Interpreting the Data_

- Each line/series: Represents the p95 latency (typically in milliseconds or seconds) for a specific persistence operation (e.g., “InsertRecord”, “UpdateWorkflow”, etc.).
- Low/stable values: Indicate consistently fast responses from your storage layer—no significant bottlenecks for most users.
- Rising trend: May suggest performance degradation for certain operations, increased backend load, or underlying database slowness.
- Sudden spikes: Often point to resource exhaustion, locked tables/rows, network issues, or other infrastructure problems.
- Only some operations spiking: Targeted issue affecting particular queries, tables, or features.
- All operations spiking: Suggests a general database problem, possibly impacting overall system responsiveness.

This panel is valuable for monitoring storage subsystem health, ensuring user-facing performance objectives are met, and proactively identifying latency outliers before they impact service reliability or user experience.


#### Service Client - All

**Requests Vs Errors ($Service -> $Client)** displays and compares the rate of all outgoing internal RPC requests sent by your service to backend system components (“client requests”) with the rate of errors encountered making those requests (“client errors”). This panel gives you a top-level view of both traffic volume and communication reliability between internal services.

_Interpreting the Data_

1. **(A)** client_requests — Total number of outgoing requests sent from your service to backend components per second (smoothed over 5 minutes).
    - Rising values: Reflect increased workload, new feature activation, or traffic spikes within the platform.
    - Sudden drops: May signal outages, network partitioning, or loss of internal workflows/tasks.
    - Flatlining: Suggests either normal inactivity, upstream issues, or a critical communication failure.

1. **(B)** client_errors — Total number of errors encountered when making internal client requests per second (smoothed over 5 minutes).
    - Low/near zero values: Indicates healthy communication with backend services.
    - Spikes: Point to instability or disruption in downstream services, networking, or protocol mismatches; may correlate with incident windows or system upgrades.
    - Persistent elevation: Highlights systemic issues or misconfigurations in underlying platform infrastructure.

Requests and Errors together: Allows you to correlate changes in traffic with error events. For example, if both spike, it might indicate overload; if requests stay stable but errors rise, there could be a downstream outage or business logic regression.

This panel is essential for monitoring the overall health of your service’s outgoing (dependent) calls to other system components, quickly identifying reliability issues, and supporting root cause analysis during incidents or periods of degraded performance.

---

**Client Requests Per Operation ($Service -> $Client)** displays the rate at which your service is making internal RPC requests to backend system components (such as History and Matching services), broken down by operation. This panel helps you observe traffic patterns and usage intensity for each critical inter-service API over time.

_Interpreting the Data_

- Each line/series: Represents the rate of requests per second for a specific internal RPC operation, such as:
    - HistoryClientStartWorkflowExecution: Requests to initiate new workflow executions in the History service.
        - High rate indicates frequent workflow creation activity; sudden drops could mean blocked workflow starts or upstream issues.
    - MatchingClientCancelOutstandingPoll: Cancel requests for outstanding polls to the Matching service.
        - Spikes may indicate worker restarts, application rebalancing, or error-handling retries.
    - MatchingClientListNexusEndpoints: Requests to retrieve a list of Nexus endpoints from the Matching service.
        - Patterns here may track topology changes, client registration events, or discovery refresh intervals.
    - MatchingClientPollActivityTaskQueue: Requests to poll for available activity tasks.
        - High sustained rates indicate active task consumers; sudden gaps may signal worker outages or load shifting.
    - MatchingClientPollWorkflowTaskQueue: Requests to poll for workflow tasks.
        - High or spiky rates may correlate with workflow load surges, new deployments, or aggressive workload partitioning.
- Stable trends: Show regular, healthy internal traffic for each operation—good for capacity planning and verifying expected usage.
- Spikes in specific operations: May signal an incident, code change, or surge in demand for a particular subsystem.
- Sudden drops to zero: Could be symptomatic of upstream failures, disabled features, or outages affecting the ability to make calls.
- Comparing operations: Reveals which internal APIs are relied on most heavily, helping prioritize monitoring, scaling, and optimization efforts.

This panel is vital for understanding service-to-service communication patterns, monitoring the health and responsiveness of key platform components, and quickly identifying disruptions or excess load on specific internal APIs.

---


**Errors By Operation ($Service -> $Client)** displays the rate of errors encountered when your service acts as a client making internal RPC calls to key backend system components (such as History and Matching services). Each series represents a specific system-to-system API operation, providing visibility into the reliability and health of internal service communication.

Interpreting the Data

- Each line/series: Represents the error rate for a specific internal client RPC operation, such as:
    - HistoryClientStartWorkflowExecution: Errors encountered when requesting to start a new workflow execution via the History service.
        - Spikes or persistently elevated values may point to workflow service issues, misconfigurations, or request malformation.
    - MatchingClientListNexusEndpoints: Errors when listing Nexus endpoints from the Matching service.
        - Spikes could indicate Matching service instability or network issues.
    - MatchingClientPollActivityTaskQueue: Errors polling for activity tasks to assign to workers.
        - Spikes often signal Matching service load, unavailability, or client-side issues.
    - MatchingClientPollWorkflowTaskQueue: Errors when polling for workflow tasks.
        - Patterns here may reflect workflow cluster overload, scaling bottlenecks, or deployment disruptions.

- Persistent errors for a single operation: Indicates a chronic problem affecting that inter-service RPC, needing targeted investigation.
- Spikes for specific operations: Often correspond to transient service outages, scaling events, configuration problems, or recent deployments.
- Multiple operations spike: May suggest a platform-wide incident or shared dependency failure impacting several system services at once.
- Flatlining/zero errors: Indicates those particular service calls are currently healthy.


This panel is essential for monitoring the health and diagnosing issues within distributed, microservice-based platforms, enabling fast root cause analysis of platform reliability, and informing where engineering efforts should be focused to maintain overall system stability.

---


**Client Latency ($Service -> $Client) **shows the 95th percentile (p95) latency for RPC (remote procedure call) operations made from your service to key internal components (such as History and Matching services). This panel tracks the response time for critical inter-service API calls, broken down by operation, and is crucial for diagnosing performance issues within your orchestration platform.

_Interpreting the Data_

- Each line/series: Represents the p95 latency (in ms or s) for a specific internal RPC operation from your service to a dependent backend service, including:
    - HistoryClientStartWorkflowExecution: Time it takes to start a workflow execution via the History service.
        - Spikes may be linked to History service overload, state storage issues, or coordination overhead.
    - MatchingClientCancelOutstandingPoll: Response time when canceling outstanding poll requests to the Matching service.
        - Spikes could indicate Matching service backpressure or delays processing poll operations.
    - MatchingClientListNexusEndpoints: Latency for fetching a list of Nexus endpoints from the Matching service.
        - Increased latency may point to discovery, database, or Matching service performance problems.
    - MatchingClientPollActivityTaskQueue: Time to poll for an activity task assignment.
        - Spikes may indicate Matching worker resource contention or cross-service networking issues.
    - MatchingClientPollWorkflowTaskQueue: Latency for polling the workflow task queue.
        - Long delays suggest workflow queue congestion, Matching service load, or downstream worker lag.
- Low/stable p95 values: Operations are performing well for nearly all requests; system is healthy.
- Rising p95 latency for a specific operation: Indicates a growing performance issue tied to a particular API call, possibly requiring targeted investigation.
- Sudden spikes (short-term): Often correlate with incident events, service restarts, resource exhaustion, or deployments.
- Simultaneous spikes in multiple operations: May indicate shared infrastructure problems (e.g., network, database, or cluster-wide contention).

This panel is essential for proactively identifying bottlenecks and performance regressions in your internal microservices communication, supporting SLO/SLA enforcement, and alerting engineering teams before latency impacts reliability or the end-user experience.

---



## Exercise conclusion






