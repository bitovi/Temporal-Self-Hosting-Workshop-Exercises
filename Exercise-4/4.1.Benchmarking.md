# 4.1: Benchmarking
Benchmarking is important to Temporal because it allows the team to systematically measure and evaluate the performance, scalability, and reliability of the Temporal platform. By running benchmarks, you can identify bottlenecks, optimize core components, and ensure the system meets performance expectations for real-world workloads.


In this exercise we are going to work with Temporal's benchmarking tool as it's easy to setup and get things running.


### Pause to consider:
    1. Before we create load give the system 2 minutes to stabliize
        - What is the persistence latencies?
        - What are the service latencies?
        - How many persistence requests are there without any load?
            - What does that tell us?


From the project root run the following within a terminal window/tab:
```bash
kubectl apply -f ./benchmark-deployment.yaml
```
This will create two deployments: One that will connect to the Temporal service and act as a worker and the other which will create workflow requests that the first worker will act on.


It may be tempting to only look at workflow success vs workflow failure but this isn't always the case. Temporal is durable and execution often times have retry logic, as a result success vs failure doesn't always tell the whole story of how the Temporal service is performing. This is going to measure the **number of state transitions** which represents Temporal writing to its persistence backend, which is a reasonable proxy of how much work Temporal itself is doing. This is a handy metric to leave up and follow as we continue through this section.

### Pause to consider:
    1. The load generator ensures 3 workflows are queued at all times, with only a single worker handling the load generated:
        - what is the number of state transitions we are seeing
        - What is the latency of the shards?
        - How many "workflow successes" are there on average?
        - Compare the number of workflow success to the state transitions
            - Why might the state transitions provide more meaningful data then purely looking at success vs failures?


## Scale up the concurrent activities generated


```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=6
```

### Pause to consider:
    1. We've double the workflows sitting idle
        - What will change with our counts such as state transitions and signal request?
        - What will change with our latency?
    2. Why did History Shard lock latency jump?
        - How could we reduce it?


## Increase the History service replicas
```bash
kubectl scale deployment my-temporal-history --replicas=2
```

### Pause to consider:
    1. What did this do to the shard balance?
    2. What did this do to our Shard lock latency?
        - During the first 2 minutes
        - After the first 2 minutes?
    3. What caused our latency to spike like that even though we spread the compute out?



## Exercise cleanup

We've likely filled the database. Instead of deleting things, we are going to restart from scratch.


```bash
helm uninstall my-temporal;
kubectl delete deployment benchmark-soak-test;
kubectl delete deployment benchmark-workers;
```

Wait until your Temporal instance is delete and then recreate it with
```bash
helm upgrade \
    --install \
    my-temporal temporalio/temporal \
    --set server.replicaCount=1 \
    --set cassandra.config.cluster_size=1 \
    --set elasticsearch.replicas=1 \
    --set prometheus.enabled=true \
    --set grafana.enabled=true \
    --timeout 15m
```

Your grafana admin password has changed, so you'll need to get the new one:
```bash
kubectl get secret my-temporal-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

Finally create a service account with editor permissions and readdd the Bitovi dashboard

```bash
curl -X POST "localhost:3000/api/dashboards/db" \
  -H "Authorization: Bearer <bearer-token>" \
  -H "Content-Type: application/json" \
  -d @bitovi-expanded-dashboard.json
```