# 4.1: Benchmarking
Benchmarking is important to Temporal because it allows the team to systematically measure and evaluate the performance, scalability, and reliability of the Temporal platform. By running benchmarks, you can identify bottlenecks, optimize core components, and ensure the system meets performance expectations for real-world workloads.


In this exercise we are going to work with Temporal's benchmarking tool as it's easy to setup and get things running.


### Pause to consider:
    1. Before we create load give the system 2 minutes to stabliize
        - What is the persistence latencies?
        - What are the service latencies?
        - How many persistence requests are there without any load?
            - What does that tell us?


From the project root run the following within a terminal window/tab:
```bash
kubectl apply -f ./benchmark-deployment.yaml
```
This will create two deployments: One that will connect to the Temporal service and act as a worker and the other which will create workflow requests that the first worker will act on.


It may be tempting to only look at workflow success vs workflow failure but this isn't always the case. Temporal is durable and execution often times have retry logic, as a result success vs failure doesn't always tell the whole story of how the Temporal service is performing. This is going to measure the **number of state transitions** which represents Temporal writing to its persistence backend, which is a reasonable proxy of how much work Temporal itself is doing. This is a handy metric to leave up and follow as we continue through this section.

### Pause to consider:
    1. The load generator ensures 3 workflows are queued at all times, with only a single worker handling the load generated:
        - what is the number of state transitions we are seeing
        - What is the latency of the shards?
        - How many "workflow successes" are there on average?
        - Compare the number of workflow success to the state transitions
            - Why might the state transitions provide more meaningful data then purely looking at success vs failures?


## Scale up the concurrent activities generated


```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=6
```

### Pause to consider:
    1. We've double the workflows sitting idle
        - What will change with our counts such as state transitions and signal request?
        - What will change with our latency?
    2. Why did History Shard lock latency jump?
        - How could we reduce it?

> [!TIP] 
> Wait a minute or two and observe the changes

## Scale it up to 9 and ask the same questions
```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=9
```

## Scale it up to 15 and ask the same questions
```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=15
```

### Pause to consider:
1. What is different this time? Are we seeing any more state transitions than we did at 9?


### Turn off the generator for now
```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=0
```

## Once you reach this point..
Put your hand-up

### Discussion 1
What did we just learn?

- How many state transitions did we see?
- How many total transitions did we see?
- How many CPU cores were used by the history service at its peak?
- What was the schedule to start latency?
- What was our history shard lock latency?
- What does all of this tell us, what insights have we just gained about our system?

