# 4.4: Benchmarking Part 4
In the last exercise part we saw what the upper bounds was for our system and worker fleet were. We scaled horizontally scaled our services up but we saw no additional improvements..
This leads use to believe that this must be a shard count issue, and there is only one way to find out.. scale, test and measure!

## Increase the shard count
We need to increase the shard count from 512 to 1536. To do so we need to perform a system reset.

1. Use helm to uninstall the temporal deployment
2. Use the command from 3.1 and add `--set server.config.numHistoryShards=1536`

## Next scale up the resources
```bash
kubectl scale deployment/my-temporal-frontend --replicas=2;
kubectl scale deployment/my-temporal-history --replicas=3;
kubectl scale deployment/my-temporal-matching --replicas=2;
kubectl scale deployment/my-temporal-worker --replicas=2;
```

## Retrieve the Grafana admin password
Use the instructions from 3.1 and 3.2 to reset the grafana dashboards.

## Scale the load to 15 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=15;
kubectl scale deployment benchmark-workers --replicas=3;
```

### Pause to consider:
1. I've purposely left a critical step from above out, if you haven't done it yourself already it's likely that you don't have anything being generated. Think about what we perhaps forgot to do and if you get stuck let us know and we can come over and help.

> [!TIP]
> You may need to reset the load generator once you figure it out!

You may need to restart the benchmark-workers once you complete the above

```bash
kubectl delete pod $(kubectl get pods | grep benchmark-workers | awk '{print $1}')
```

## Measure
In our previous test we saw the upper bounds of our system, which was ~around 2000 state transitions per second.  We increased our worker fleet and saw marginal improvements. We increased our system resource and saw no real improvements. With the additional shards now available to the workers, we should see our system effectively handle the top end load!


Give the system 2 minutes to stablize

### Pause to consider
1. During our last test we saw spikes to ~2000 then slow returns back to lower numbers (aroun 1100), what do we see now?
2. Is the history service using more or less CPUs?
3. Is our shard lock and history lock latency better?


## Scale the load to 25 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=25
```

### Pause to consider
1. Investigate why our system is seeing performance degradation and not improving..


## Stop the load
Wait about 5 minutes before stopping the load.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=0;
kubectl scale deployment benchmark-workers --replicas=1;
```

## Discussion
Let the instructor know once you get here.

The question is: Why did we see performance degradation? 

## Clean up and try again

Let's delete the `default` namespace, then cycle the history nodes, then recrease the `default` namespace.


## Scale the load to 25 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=25;
kubectl scale deployment benchmark-workers --replicas=3;
```

### Pause to consider
1. During our last test we saw spikes to ~2000 then slow returns back to lower numbers (aroun 1100), what do we see now?
2. Is the history service using more or less CPUs?
3. Is our shard lock and history lock latency better?


## Stop the load
Wait a few minutes before stopping the load.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=0;
kubectl scale deployment benchmark-workers --replicas=1;
```