# 5.1: Using Omes for load simulation

A major hurdle for operators is the ability to uncover whether a system is working at peak efficiency. For small scale systems this can be mostly ignored, but for systems that are crucial for high-priority operations, the need for quantitative measurements that a system is working within reasonable boundaries is required. Load simulation when combined with metric visualization is a means of bench-marking the capacity of a system in a controlled environment.

Temporal offers a simple out-of-the-box load generator for Temporal called Omes. It’s simple to use and comes with standardized test scenarios that can be used or built upon.

The tests are flexible, with the ability to modify the duration, SDK used and load generated through command line flags however they should be expanded upon for live usage when the Temporal operator team is ready to take their operational excellence to the next level.


**Downloading Omes:** We'll begin by cloning Omes to our local machine. `git clone https://github.com/temporalio/omes.git`

Then we will ensure our Temporal default namespace is created and ready as it was likely wiped out when we scaled our database to 0 in the previous exercise.
```bash
temporal operator namespace create default
```

## Simulate steady load
This scenario steadily creates a specific, user-defined rate of workflow state transitions per second, simulating a controlled, constant workflow load on the cluster to measure sustained throughput.

This should be considered the baseline test as to establish the max throughput of a system before moving on to the other more niche scenarios. If you are only running a single simulation, this is the simulation you would want to use.

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=3 &>> ../logs/state_transitions_steady.log
```

This will create steady load for the next 5 minutes, so while that is going on let's jump back into our Grafana UI and see what has changed. 

Let's begin by opening the **Temporal Server Metrics** dashboard, scrolling to the row **Overview - All** and taking a look at the **Service Latency**. What we want to see is a steady latency rate that are within normal bounds. Remember we are using a small local setup so these metrics will not represent what a live environment would, for example a 1 minute latency is high, but it does show a consistent trend. If we were to see this spike we'd have a good starting point to begin digging into a potential issue.

Now open the **Temporal UI** and refresh the homepage a few times, notice how there is a consistent but low number of workflows starting then completing without error. Though the workflows are small again we are looking for consistency. As the 5 minute timer hits you'll notice that all workflows have completed as expected with none hanging. One thing to note is that the Omes tool uses our local machine as the worker, in a live environment, it is recommend that you modify the Omes tool to use dedicated workers as to avoid I/O issues and to bring the benchmark environment in line with the live environment.

Heading back to Grafana open the **Cluster Monitoring for Kubernetes** dashboard and find the **Network I/O pressure** panel. As we can see during our testing time the pressure on the system increased. Note as well that the traffic sent is slightly higher than the traffic receieved. This is because the worker, our local machine in this case, was polled for state updates before returning to a normalized amount after the test finishes. Additionally note that the traffic of the worker was not captured, for that we would need to expand the metrics scrapped to include workers which is outside the scope of this workshop.

Next open the **History Service** dashboard, let's first look at the **Requests Per Operation** panel, in the first panel we can see that the requests rise, peak then descend, this is expected behavior as our benchmark test created load for 5 minutes and then stopped. In the panel of the same name beside it we can click on the event type, such as **RecordWorkflowTaskStarted**, we can see it too follows the same pattern of requests made rising, peaking and then falling as the test finished. Use these types of metrics to establish a baseline.

Next scroll down to **Active Timer Task Total Latency** and **Active Time Task Queue Latency**, these provide important insight into the latency of the active tasks running, again we see rise, peak, fall but notably, once the test finishes we notice a complete stop to some values, they don't drop to 0 instead they stop all together. This is because that specific event is no longer producing scrapable metrics and thus is effectively nil. This again sets the precident of a baseline. 

Finally scroll down to **Acquire Shards Count** and **Acquire Shards Latency** panels, in the former we see that during the test there was a spike to a consistent value then as the test finished a drop-off (even then the numbers really aren't that extreme from each other). In the former we can see consistent peak-hold-drop. The information to be inferred is related to the History service updating the persistence datastore as the tasks run and complete.

Now let's see how a larger load will be handled.

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=60 &>> ../logs/state_transitions_steady.log
```

Opening the Temporal Server Metrics dashboard, scrolling to the row **Overview - All** and taking a look at the **Service Latency**. Click through the events and notice that though there are changes, those changes are consistent, nothing is spiking or seemingly overwhelming, this indicates to us that our system is handling this load effectively. We can further verify this by heading to the Temporal UI and refresh the page, notice how once again there are workflows being started and completed, but nothing is piling up, for every started workflow there is a completed workflow.

Within the Cluster monitoring dashboard we can once again see that the I/O pressure to the system jumped, in this case significantly more than before, but once the test finishes it will again return to normal lower levels as we expect. If it didn't that may indicate that either the worker isn't responding in the case of bytes sent is increasing but received is not, or alternatively the worker poller has stopped if vice versa. 

Back in the History dashboard we can see the **Acquire Shards Count** has once again jumped up as the History pod began updating the Persistence datastore but to a consistent level which will taper out over time. Scrolling down we can see in the **Requests vs Errors** panel that we maxed out at around 500 requests to the persistence datastore with no reported errors and if we wanted to we could further breakdown the events of the requests in the panel beside it or the latency of those operations beneath. 


## Exercise 1 - Familiarizing yourself with normal operations
 
We are going to run this test again, during the test find the answers to the following:

1. What persistence operations are the most frequently used during this type of test and what is the latency of that operation
2. How can we determine whether errors are occuring?

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=60 &>> ../logs/state_transitions_steady.log
```


## Exercise 2 - Causing failure during the test
Let's now see what happens when we cause failure during a test. 

We are now going to simulate an even higher load. During this exercise attempt to determine the following:
1. What happened to our shard distribution?
2. Where can we see the service errors?
3. What service errors did we see?
4. Was our "Service Availability" consistent?
5. What error did the History service report?

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=3 &>> ../logs/state_transitions_steady.log &;
echo "Sleeping for 60 seconds"; sleep 60;
kubectl scale deployment my-temporal-history --replicas=0;
echo "Sleeping for 30 seconds"; sleep 30;
kubectl scale deployment my-temporal-history --replicas=1;
```

## Exercise Conclusion

In this exercise, you took your Temporal observability and operations knowledge to the next level by systematically simulating and analyzing load on your cluster using Omes, Temporal’s official load generator tool. Through hands-on experimentation and metric exploration, you:

- Benchmarked Baseline Performance: By running steady-state load tests, you identified your system’s normal throughput and latency characteristics, giving you a reference point for future scaling and troubleshooting.
- Visualized System Behavior Under Load: You used Grafana dashboards to correlate simulated workload with service latency, shard distribution, active task queues, and network I/O—all critical indicators of cluster health.
- Explored Operational Limits: By increasing load, you observed the system’s ability to maintain stability, revealed potential bottlenecks, and learned where and how to spot emerging instability before it becomes a customer-facing issue.
- Simulated and Diagnosed Failure Scenarios: You introduced real-time disruptions during load (such as scaling down the History service) and practiced interpreting resulting alerts, error spikes, and service disruptions across multiple dashboards.

Key Takeaways:

1. Conducting controlled load tests is the most reliable way to build confidence in your deployment’s true throughput and error-handling capabilities.
1. Observability isn’t just about dashboards, but about tying metrics to real operational questions—like “Is the system keeping up?”, “Where do errors first appear?”, and “What happens when something goes wrong at scale?”
1. Real-world load testing helps solidify your incident response muscle memory, so you’re ready when unexpected spikes or outages occur in production.


Now that you have firsthand experience with both normal and stressed workloads, the final step in becoming operationally confident with Temporal is to hone your skills in incident detection and resolution under load. In the forthcoming exercise, you'll focus on [troubleshooting workflow failures, recovering from high-stress conditions, and further refining your monitoring](./5.2.Scaling-Temporal-Services.md) for true production readiness.
