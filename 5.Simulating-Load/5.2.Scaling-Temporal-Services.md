# Scaling Temporal Services
In the last section we spent time getting more familiar with metric data visualization and leanered about what consistency means in our current system. It's now time to make changes to our system capacity and understand how to interpret how the server responds.


## Exercise 1 - Add load to the system
In this exercise we want to really push the amount of load the test is generating and see how our Temporal service responds. We'll explore what we can determine with our dashboards and UI and finally we'll manually stop any remaining workflows that became stuck during our test.

You have one question to answer:
1. What are the indicators that something is wrong and our Temporal Service isn't keeping up?

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```


### Answers
1. In the Temporal UI the "Running" workflows are stacking up faster than the completed are causing a backlog of actions that the Temporal Server needs to perform. This is an indicator of worker overload=
2. In the Temporal Server Metrics dashboard, within the **Workflow tasks** and **Activities** panels, up until this point the number of completed tasks has always been at least double the started value, this means that on we complete two activities for every 1 created. With this test however, the started amount has begun to exceed the completed values, resulting in backlogging of requests. In a live environment this **could** be dealt with as the worker given enough time will execute and resolve the backlog of items once requests decrease, however this is still highlighting what high worker stress would look like.


It's likely that you now have stuck workflows since the worker is stopped once the test finishes. Scroll to the bottom of this page for information on how to terminate stuck workflows. Before doing so make a note of how many workflows had to be terminated.


## Exercise 2 - Increase the replica count of all primary services
Let's now increase the amount of replicas we have for our Frontend, History and Matching service. Doing so will result in a shard rebalance that we will want to allow to normalize before proceeding. 

Perform the following command:
```bash
kubectl scale deployment my-temporal-frontend --replicas=2;
kubectl scale deployment my-temporal-history --replicas=3;
kubectl scale deployment my-temporal-matching --replicas=2;
```

Within the **Temporal Server Metrics** dashboard wait until the **Shard Rebalancing** and **Shard Distribution** panels normalize, it should take 2 minutes. Afterward, let's redo that test and see if anything changes.

Let's test whether increasing the replicas of our crucial services results in better performance.

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```

**Answer:** Yes and no:
_Yes:_
    1. Looking at most of the operation metrics, we see a more gradual rise then fall instead of peaks and holds, this indicates that workflows/activities are being worked on in parallel resulting in smoother overall operations.
    2. We see a increase in shard latency, if we look at the balancing we can see that each history service replica has ~170 shards. The result is that the history pods are being throttled, they have the compute to be doing more operations but the shard count is limiting how many they are able to perform.
_no:_
    1. We still see the build up of a backlog, this is because of our local machine acting as a worker is simply not able to execute on the workflows fast enough to finish the test.

Even with the increase in resources we saw performance loss, compare the number of hung workflows this time against last time, (during my testing) I saw a consistent increase of ~10% more hung workflows. What this tells


## Increasing the shard count
The shard count is set at server initialization and cannot be reset once Temporal has started. The recommendation from Temporal is to have ~500 shards per 1 history node. If we were to go-rogue and increase the shard count to 1000:1 then we'd see no real performance improvements as the Temporal History Service would hit a bottleneck as the history node would struggle to perform that many parallel actions, though with vertical scaling you could get more operations out of the history node. In contrast, going the opposite direction where we had ~200:1 we'd see a self-imposed limit in which the history service wouldn't be able to keep the persistence datastore close to the live data state.


## Stuck workflows
Because our local machine is being treated as a worker and there is a time limited duration for the test, it is possible that the test concludes before the workflows can all finish. In this case we can terminate those tasks by running the following:

```bash
temporal workflow terminate --query "WorkflowType=\"kitchenSink\""
```

This is why creating a dedicated worker for benchmarking is important, as it can continue to act on the workflows regardless of time duration. The test would still fail but the workflows would not become stuck.

## System OOM
If within the **Cluster Monitoring for Kubernetes** the Cluster Memory usage is above 90%, it's best to uninstall and reinstall the Temporal service. In a live environment this is where it would be best to implement memory clawback, increases, etc to better fit your Temporal Server requirements.

First uninstall the Temporal install with:
```bash
helm uninstall my-temporal
```

Afterwards follow the instructions in [4.1](../4.Observability-and-Monitoring/4.1.Adding-monitoring-tools-Prometheus-and-Grafana.md).


## # Scaling Temporal Services
In the last section we spent time getting more familiar with metric data visualization and leanered about what consistency means in our current system. It's now time to make changes to our system capacity and understand how to interpret how the server responds.


## Exercise 1 - Add load to the system
In this exercise we want to really push the amount of load the test is generating and see how our Temporal service responds. We'll explore what we can determine with our dashboards and UI and finally we'll manually stop any remaining workflows that became stuck during our test.

You have one question to answer:
1. What are the indicators that something is wrong and our Temporal Service isn't keeping up?

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```


### Answers
1. In the Temporal UI the "Running" workflows are stacking up faster than the completed are causing a backlog of actions that the Temporal Server needs to perform. This is an indicator of worker overload=
2. In the Temporal Server Metrics dashboard, within the **Workflow tasks** and **Activities** panels, up until this point the number of completed tasks has always been at least double the started value, this means that on we complete two activities for every 1 created. With this test however, the started amount has begun to exceed the completed values, resulting in backlogging of requests. In a live environment this **could** be dealt with as the worker given enough time will execute and resolve the backlog of items once requests decrease, however this is still highlighting what high worker stress would look like.


It's likely that you now have stuck workflows since the worker is stopped once the test finishes. Scroll to the bottom of this page for information on how to terminate stuck workflows. Before doing so make a note of how many workflows had to be terminated.


## Exercise 2 - Increase the replica count of all primary services
Let's now increase the amount of replicas we have for our Frontend, History and Matching service. Doing so will result in a shard rebalance that we will want to allow to normalize before proceeding. 

Perform the following command:
```bash
kubectl scale deployment my-temporal-frontend --replicas=2;
kubectl scale deployment my-temporal-history --replicas=3;
kubectl scale deployment my-temporal-matching --replicas=2;
```

Within the **Temporal Server Metrics** dashboard wait until the **Shard Rebalancing** and **Shard Distribution** panels normalize, it should take 2 minutes. Afterward, let's redo that test and see if anything changes.

Let's test whether increasing the replicas of our crucial services results in better performance.

```bash
go run ./cmd run-scenario-with-worker --scenario state_transitions_steady --language go --duration 5m --option state-transitions-per-second=600 &>> ../logs/state_transitions_steady.log
```

**Answer:** Yes and no:
_Yes:_
    1. Looking at most of the operation metrics, we see a more gradual rise then fall instead of peaks and holds, this indicates that workflows/activities are being worked on in parallel resulting in smoother overall operations.
    2. We see a increase in shard latency, if we look at the balancing we can see that each history service replica has ~170 shards. The result is that the history pods are being throttled, they have the compute to be doing more operations but the shard count is limiting how many they are able to perform.
_no:_
    1. We still see the build up of a backlog, this is because of our local machine acting as a worker is simply not able to execute on the workflows fast enough to finish the test.

Even with the increase in resources we saw performance loss, compare the number of hung workflows this time against last time, (during my testing) I saw a consistent increase of ~10% more hung workflows. What this tells


## Increasing the shard count
The shard count is set at server initialization and cannot be reset once Temporal has started. The recommendation from Temporal is to have ~500 shards per 1 history node. If we were to go-rogue and increase the shard count to 1000:1 then we'd see no real performance improvements as the Temporal History Service would hit a bottleneck as the history node would struggle to perform that many parallel actions, though with vertical scaling you could get more operations out of the history node. In contrast, going the opposite direction where we had ~200:1 we'd see a self-imposed limit in which the history service wouldn't be able to keep the persistence datastore close to the live data state.


## Stuck workflows
Because our local machine is being treated as a worker and there is a time limited duration for the test, it is possible that the test concludes before the workflows can all finish. In this case we can terminate those tasks by running the following:

```bash
temporal workflow terminate --query "WorkflowType=\"kitchenSink\""
```

This is why creating a dedicated worker for benchmarking is important, as it can continue to act on the workflows regardless of time duration. The test would still fail but the workflows would not become stuck.

## System OOM
If within the **Cluster Monitoring for Kubernetes** the Cluster Memory usage is above 90%, it's best to uninstall and reinstall the Temporal service. In a live environment this is where it would be best to implement memory clawback, increases, etc to better fit your Temporal Server requirements.

First uninstall the Temporal install with:
```bash
helm uninstall my-temporal
```

Afterwards follow the instructions in [4.1](../4.Observability-and-Monitoring/4.1.Adding-monitoring-tools-Prometheus-and-Grafana.md).


## Exercise conclusion

In this exercise, you explored the practical impact of scaling core Temporal services and adjusting cluster architecture on the system’s capacity and resilience. By testing your cluster under heightened loads and modifying resource allocation, you:

- Identified Bottlenecks Under Heavy Load: By dramatically increasing the workflow generation rate, you witnessed firsthand what system strain looks like, such as running workflows outpacing completions and a clear backlog building in the Temporal UI, indicating when the worker infrastructure cannot keep up.
- Used Metrics to Spot Resource Constraints: You analyzed Grafana dashboards to pinpoint where the system was lagging, observing key changes in workflow/activities task ratios, shard distribution, and latency. This process highlighted the need to correlate dashboard signals with “real” operational slowdowns.
- Scaled Out Core Services: By increasing the replica counts of Frontend, History, and Matching services, you observed both the benefits and limits of horizontal scaling. Work was distributed more evenly across pods, but new bottlenecks emerged, such as shard-level throttling or worker limitations, illustrating that not all performance issues can be solved by simply adding replicas.
- Understood the Role of Shards and Capped Scalability: You discussed the impact of shard count and server architecture, learning why simply increasing shards doesn’t always provide linear improvements and how improper sizing can impose self-inflicted constraints.
- Worked Through Hung Workflows and Remediation: You experienced the downstream effects of heavy load and incomplete test runs—such as workflows stuck in progress—and practiced remediating these situations by manually terminating workflows, emphasizing the importance of dedicated benchmarking resources.
- Prepared for System-Level Risks: You also handled system-level risks like potential out-of-memory scenarios, reinforcing the value of cluster monitoring and recovery strategies as part of operational excellence.

Key Takeaways:

1. Scaling Temporal requires more than just adding compute; understanding the interplay between workflow throughput, worker capacity, shard distribution, and live system metrics is essential for effective architectural decisions.
1. Observability enables not just detection of slowdowns or errors, but a root-cause mindset: Knowing why the backlog forms or where capacity is being exhausted is the difference between short-term fixes and lasting improvements.
1. Realistic load and scaling experiments are essential for validating “real world” readiness—highlighting that resolving bottlenecks demands holistic awareness of both application and infrastructure layers.

Armed with this hands-on scaling experience, you now possess deeper operational insight into how Temporal’s architecture responds to high-stress conditions, and how to interpret the metrics and behaviors that surface under load.

The next step toward production competence is ...





