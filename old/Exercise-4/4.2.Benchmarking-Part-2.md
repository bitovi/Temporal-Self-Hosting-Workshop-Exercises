# 4.2: Benchmarking Part 2
In the last exercise part we saw what the upper bounds was for the system. Now let's see what happens if we make our worker fleet larger!

## Before we begin let's clean out all the test data we just generated

Use the Temporal CLI to delete the **default** namespace. Afterwards run

```bash
kubectl delete pod $(kubectl get pods | grep my-temporal-history | awk '{print $1}')
```

Recreate the namespace once the History service refreshes and then reset the workers:

```bash
kubectl delete pod $(kubectl get pods | grep benchmark-workers | awk '{print $1}')
```

## Scale the load to 15 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=15
```

Give things a minute or two to stablize.


## Next let's scale our worker fleet to 3

```bash
kubectl scale deployment benchmark-workers --replicas=3
```

Give things a minute or two to stablize


### Pause to consider
1. Are we getting more state transitions per second?
2. Have any other metrics improved or worsened?


## Stop the test

```bash
kubectl scale deployment benchmark-soak-test --replicas=0;
```