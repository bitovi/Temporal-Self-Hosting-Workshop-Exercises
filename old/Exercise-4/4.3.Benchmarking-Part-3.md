# 4.3: Benchmarking Part 3
In the last exercise part we saw what the upper bounds was for the system and worker fleet were. Now let's scale our services and try one more time.

## Before we begin let's clean out all the test data we just generated

Use the Temporal CLI to delete the **default** namespace. Afterwards run

```bash
kubectl delete pod $(kubectl get pods | grep my-temporal-history | awk '{print $1}')
```

Recreate the namespace once the History service refreshes and then reset the workers:

```bash
kubectl delete pod $(kubectl get pods | grep benchmark-workers | awk '{print $1}')
```

## Scale the services

```bash
kubectl scale deployment/my-temporal-frontend --replicas=2;
kubectl scale deployment/my-temporal-history --replicas=3;
kubectl scale deployment/my-temporal-matching --replicas=2;
kubectl scale deployment/my-temporal-worker --replicas=2;
```

### Pause to consider
1. We just scaled our history service, what's going on with our history shards?
2. What is the new shard balance across our 3 pods?
    - What is the relation to the shard count we set in the config?


## Scale the load to 15 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl scale deployment benchmark-soak-test --replicas=1
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=15
```

Give things a minute or two to stablize.


## Next let's scale our worker fleet to 3

```bash
kubectl scale deployment benchmark-workers --replicas=3
```

Give things a minute or two to stablize


### Pause to consider
1. Are we getting more state transitions per second?
2. Have any other metrics improved or worsened?



## Scale the load to 25 requests
Let's scale our load generated back to 15 concurrent workflows in the queue. Remember this was the upper bounds of where it was a moment ago.

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=25
```

Give things a minute or two to stablize.

### Pause to consider
1. Are we getting more state transitions per second?
2. Have any other metrics improved or worsened?



## Stop the test

```bash
kubectl set env deployment/benchmark-soak-test CONCURRENT_WORKFLOWS=0;
kubectl scale deployment benchmark-workers --replicas=1;
```