# 4.3: Simulating failures and viewing retry and alerting behavior

In our last exercise we took time to read through the first responder dashboard: **Temporal Server Metrics**. This should always be the first place we look when we want to begin diagnosing or investigating our Temporal service health and performance.


## Exercise setup
In your Grafana UI, on the top right click the down pointing arrow beside the refresh button, in the dropdown menu select **auto**. To the left of that ensure the timescale selected is **Last 5 minutes**.


## Exercise 1 - Create an alert
Let's begin by creating  Prometheus alert that will sound off when our system performance becomes degragaded. Prometheus alerts aren't a fully-fledged notfication solution but it does provide a warning for operators when something in their self-hosted Temporal service isn't right.


1. In the left hand panel open the **Alerting > Alert Rules**
2. On the top right click the **New Alert Rule** button
3. Enter the alert rule name:  **Service Availability below 75%**
4. In the define query and alert condition
    - Select **code** and paste the following: `100 - (sum(rate(service_errors[2m]) OR on() vector(0)) / sum(rate(service_requests[2m])) * 100)`
    - Click **Options** and change Time Range to `now-1m to now`, Max data points to `43200` and interval to `1s`
5. Within the expressions tab
     - Within the **B Reduce** panel, remove the **B Reduce** panel by clicking the garbage can on the right
     - Within the **C Threshold** panel, update the input to A, update the **IS ABOVE** evaluation to **IS BELOW** and set the value to 75
6. Within the **Set evaluation behavior** tab
    - Click `+ New folder` and give it the name `Service_Availability`
    - Click `+ New evaluation group` and give it the name `Service_Availability_Group`, Set the Evaluation interval to 10s
7. Within the **Configure label and notifications** tab
    - Set the **Contact point** to `grafana-default-email`
8. On the top right click `Save rule and exit`


This rule will watch the same value as the **Temporal Server Metrics dashboard > Service Availability** panel, if the value drops below 75% it will trigger this alert. Within the **Alerting > Alert Rules** open up the `Service_Availability > Service_Availability_Group`, you can see our newly created rule and assuming your Temporal service is stable and running, we will see the tags `1 normal | 10s`.

Let's now break some things and see how our dashboards respond!


## Exercise 2 - Simulate Frontend Pod Disruption
Let's begin by simulating failure within our frontend service, this will trigger our alert rule and we will see the issue witihin our **Temporal Server Metrics dashboard > Service Availability** panel.


Begin by opening the **Temporal Server Metrics dashboard > Service Availability** panel, we want to see the failure as it happens

Let's begin by scaling our frontend pod down to 0 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=0
```

Keep an eye on your Grafana **Temporal Server Metrics** dashboard, we can see that the panels have already begun to change.

1. **Temporal > Service Availability** panel: We can see that availability has dropped from 100 to NaN
2. **Overview - All > Service Requests** panels: We can see that the number of requests has dropped from a consistent ~1.5 calls to 0. 


> [!TIP]
> Temporal Server Metrics is a starting point, in a real-life event we would want to open the Frontend dashboard and dig in to the issue deeper.

Let's go back to our rule and see if that too has changed.. and it has! It has changed from `1 normal | 10s` to `1 firing | 10s`, so our alerting rule is working as expected and had we setup a true target, for example Slack, we would be receiving notifications about our Frontend service requests meeting our alert threshold.

### Exercise clean-up

Let's now scale our frontend pod down back to 1 and see what happens. Open your terminal window / tab and enter the following:

```bash
kubectl scale deployment my-temporal-frontend --replicas=1
```
