# Exercise 1.4: Deploying Temporal server components using a Helm chart.md
In this exercise we will use Helm to deploy a Temporal service to our local kubernetes cluster. We installed the Helm CLI during [exercise 1.1](../1.Preparing-Your-Environment/1.1.Installing-prerequisites.md). Helm is a package manager for Kubernetes, we will use it here to simplify and expidite our workshop but in it is highly recommended that you manage the Temporal deployment more manually in real environments.


First let's add the TemporalIO helm repo to our local helm CLI repo list
```bash
helm repo add temporalio https://temporalio.github.io/helm-charts
```

Start by running the following. This disables the persistence and visibility stores from being created.
```bash
helm upgrade \
    --install \
    my-temporal temporalio/temporal \
    --set cassandra.enabled=false \
    --set elasticsearch.enabled=false \
    --set prometheus.enabled=false \
    --set grafana.enabled=false \
    --set schema.createDatabase.enabled=false \
    --set schema.setup.enabled=false \
    --set schema.update.enabled=false \
    --set server.config.persistence.default.cassandra.port=9042 \
    --set server.config.persistence.visibility.cassandra.port=9042 \
    --timeout 15m
```

In a new terminal window/tab watch the deployment using the following (Remember to run `btw` if you've set that up):
```bash
## If watch command is available - prefered way
watch kubectl get pods
## If watch command is not available
kubectl get pods --watch
```





Now run the following command to add the persistence and visibility store services to the Temporal deployment
```bash
helm upgrade \
    --install \
    my-temporal temporalio/temporal \
    --set server.replicaCount=1 \
    --set cassandra.config.cluster_size=1 \
    --set elasticsearch.replicas=1 \
    --set prometheus.enabled=false \
    --set grafana.enabled=false \
    --timeout 15m
```

We can watch the deployment using the following:
```bash
## If watch command is available - prefered way
watch kubectl get pods
## If watch command is not available
kubectl get pods --watch
```

You should see a deployment similar to:
```bash
elasticsearch-master-0                  0/1     Running    0          12
my-temporal-admintools-6d74864f-ntwsl   1/1     Running    0          10s
my-temporal-cassandra-0                 0/1     Running    0          10s
my-temporal-frontend-98599d944-vw4lp    0/1     Init:0/2   0          10s
my-temporal-history-76dc8c9f98-fvpbw    0/1     Init:0/2   0          10s
my-temporal-matching-77b8c45d97-nkm5q   0/1     Init:0/2   0          10s
my-temporal-schema-1-fppxk              0/1     Init:0/7   0          10s
my-temporal-web-f756d58b5-mrblw         1/1     Running    0          10s
my-temporal-worker-7bc65858cb-g6rtz     0/1     Init:0/2   0          10
```


There is a lot going on, so while we wait let's break down each part and what they are doing. 

1. `elasticsearch-master` our visibility store
2. `my-temporal-admintools` This is a pod that has the admin tools, such as **temporal CLI** and the database CLIs like **temporal-cassandra-tool** and **temporal-mysql-tool**. This is used by the schema setup job to prepare the database.
3. `my-temporal-cassandra` our persistence store
4. `my-temporal-frontend` is the Temporal frontend service
5. `my-temporal-history` is the Temporal history service
6. `my-temporal-matching` is the Temporal matching service
7. `my-temporal-schema` is the Temporal schema job that prepares the persistence and visibility stores for our Temporal service. 
8. `my-temporal-web` is the Temporal UI
9. `my-temporal-worker` is the Temporal worker service

> [!NOTE]
> DON'T PANIC. It is normal for the frontend, history, matching and worker pods to go into a CrashLoop status while the schema is running. They will resolve once the schema job finishes.

